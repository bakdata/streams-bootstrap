{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"What is streams-bootstrap?","text":"<p><code>streams-bootstrap</code> is a Java library that standardizes the development and operation of Kafka-based applications (Kafka Streams and plain Kafka clients).</p> <p>The framework supports Apache Kafka 4.1 and Java 17. Its modules are published to Maven Central for straightforward integration into existing projects.</p>"},{"location":"#why-use-it","title":"Why use it?","text":"<p>Kafka Streams and the core Kafka clients provide strong primitives for stream processing and messaging, but they do not prescribe:</p> <ul> <li>How to structure a full application around those primitives</li> <li>How to configure applications consistently</li> <li>How to deploy and operate these services on Kubernetes</li> <li>How to perform repeatable reprocessing and cleanup</li> <li>How to handle errors and large messages uniformly</li> </ul> <p><code>streams-bootstrap</code> addresses these aspects by supplying:</p> <ol> <li>Standardized base classes for Kafka Streams and client applications.</li> <li>A common CLI/configuration contract for all Kafka applications.</li> <li>Helm-based deployment templates and conventions for Kubernetes.</li> <li>Built-in reset/clean workflows for reprocessing and state management.</li> <li>Consistent error-handling and dead-letter integration.</li> <li>Testing infrastructure for local development and CI environments.</li> <li>Optional blob-storage-backed serialization for large messages.</li> </ol>"},{"location":"#architecture","title":"Architecture","text":"<p>The framework uses a modular architecture with a clear separation of concerns.</p>"},{"location":"#core-modules","title":"Core Modules","text":"<ul> <li><code>streams-bootstrap-core</code>: Core abstractions for application lifecycle, execution, and cleanup</li> <li><code>streams-bootstrap-cli</code>: CLI framework based on <code>picocli</code></li> <li><code>streams-bootstrap-test</code>: Utilities for testing streams-bootstrap applications</li> <li><code>streams-bootstrap-large-messages</code>: Support for handling large Kafka messages</li> <li><code>streams-bootstrap-cli-test</code>: Test support for CLI-based applications</li> </ul>"},{"location":"javadoc/legal/jquery/","title":"Jquery","text":""},{"location":"javadoc/legal/jquery/#jquery-v361","title":"jQuery v3.6.1","text":""},{"location":"javadoc/legal/jquery/#jquery-license","title":"jQuery License","text":"<pre><code>jQuery v 3.6.1\nCopyright OpenJS Foundation and other contributors, https://openjsf.org/\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n******************************************\n\nThe jQuery JavaScript Library v3.6.1 also includes Sizzle.js\n\nSizzle.js includes the following license:\n\nCopyright JS Foundation and other contributors, https://js.foundation/\n\nThis software consists of voluntary contributions made by many\nindividuals. For exact contribution history, see the revision history\navailable at https://github.com/jquery/sizzle\n\nThe following license applies to all parts of this software except as\ndocumented below:\n\n====\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n====\n\nAll files located in the node_modules and external directories are\nexternally maintained libraries used by this software which have their\nown licenses; we recommend you read them, as their terms may differ from\nthe terms above.\n\n*********************\n\n</code></pre>"},{"location":"javadoc/legal/jqueryUI/","title":"jqueryUI","text":""},{"location":"javadoc/legal/jqueryUI/#jquery-ui-v1132","title":"jQuery UI v1.13.2","text":""},{"location":"javadoc/legal/jqueryUI/#jquery-ui-license","title":"jQuery UI License","text":"<pre><code>Copyright jQuery Foundation and other contributors, https://jquery.org/\n\nThis software consists of voluntary contributions made by many\nindividuals. For exact contribution history, see the revision history\navailable at https://github.com/jquery/jquery-ui\n\nThe following license applies to all parts of this software except as\ndocumented below:\n\n====\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n====\n\nCopyright and related rights for sample code are waived via CC0. Sample\ncode is defined as all source code contained within the demos directory.\n\nCC0: http://creativecommons.org/publicdomain/zero/1.0/\n\n====\n\nAll files located in the node_modules and external directories are\nexternally maintained libraries used by this software which have their\nown licenses; we recommend you read them, as their terms may differ from\nthe terms above.\n\n</code></pre>"},{"location":"user/monitoring/","title":"Monitoring","text":"<p>Monitoring features are provided for your applications.</p> <ul> <li>JMX Metrics Export: Applications built with <code>streams-bootstrap</code> can expose JMX (Java Management Extensions)   metrics, which provide insights into the performance and health of the Java application and the Kafka clients.</li> <li>Prometheus Integration: The Helm charts are configured to work with Prometheus, a popular open-source monitoring   and alerting toolkit. This allows you to scrape the JMX metrics and visualize them in dashboards (e.g., using   Grafana).</li> </ul>"},{"location":"user/monitoring/#monitoring-and-observability","title":"Monitoring and Observability","text":"<p>The Helm charts provide integrated monitoring and observability for Kafka applications using a combination of JMX, Prometheus, Kubernetes probes, and Services. Monitoring can be tailored from lightweight setups for development to full production stacks with Prometheus Operator.</p>"},{"location":"user/monitoring/#monitoring-mechanisms","title":"Monitoring Mechanisms","text":"Mechanism Use Case Key Values JMX remote access Direct debugging and inspection <code>jmx.enabled</code> Prometheus JMX exporter Production metrics collection <code>prometheus.jmx.enabled</code> Liveness probes Container health checks <code>livenessProbe</code> Readiness probes Traffic readiness / rollout control <code>readinessProbe</code>"},{"location":"user/monitoring/#jmx-configuration","title":"JMX Configuration","text":"<p>JMX (Java Management Extensions) provides direct access to application metrics and management operations, typically used for development and debugging.</p> <p>Enable JMX in <code>values.yaml</code>:</p> <pre><code>jmx:\n  enabled: true\n  port: 5555\n  host: localhost\n</code></pre> <p>Parameters:</p> <ul> <li><code>jmx.enabled</code>: Enable JMX port for remote access (default: <code>false</code>).</li> <li><code>jmx.port</code>: JMX port number (default: <code>5555</code>).</li> <li><code>jmx.host</code>: Host binding for the RMI server (default: <code>localhost</code>).</li> </ul> <p>When enabled, the chart configures the JVM with flags similar to:</p> <pre><code>-Dcom.sun.management.jmxremote\n-Dcom.sun.management.jmxremote.port=5555\n-Dcom.sun.management.jmxremote.local.only=false\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n-Djava.rmi.server.hostname=localhost\n</code></pre> <p>Accessing JMX metrics from a local client:</p> <pre><code>kubectl port-forward &lt;pod-name&gt; 5555:5555\njconsole localhost:5555\n</code></pre>"},{"location":"user/monitoring/#prometheus-jmx-exporter","title":"Prometheus JMX Exporter","text":"<p>For production monitoring, the Prometheus JMX Exporter runs as a sidecar container that scrapes JMX metrics from the application and exposes them in Prometheus format.</p> <p>Enable the exporter in <code>values.yaml</code>:</p> <pre><code>prometheus:\n  jmx:\n    enabled: true\n    image: bitnami/jmx-exporter\n    imageTag: 1.1.0\n    imagePullPolicy: Always\n    port: 5556\n    metricRules:\n      - pattern: \".*\"\n    resources:\n      requests:\n        cpu: 10m\n        memory: 100Mi\n      limits:\n        cpu: 100m\n        memory: 100Mi\n</code></pre> <p>Key parameters:</p> <ul> <li><code>prometheus.jmx.enabled</code>: Deploy JMX exporter sidecar (default: <code>false</code>).</li> <li><code>prometheus.jmx.image</code>: Container image for the exporter (default: <code>bitnami/jmx-exporter</code>).</li> <li><code>prometheus.jmx.imageTag</code>: Exporter image tag (default: <code>1.1.0</code>).</li> <li><code>prometheus.jmx.port</code>: HTTP port for metrics endpoint (default: <code>5556</code>).</li> <li><code>prometheus.jmx.metricRules</code>: JMX metric selection and mapping rules.</li> <li><code>prometheus.jmx.resources</code>: Resource requests/limits for the exporter container.</li> </ul>"},{"location":"user/monitoring/#metric-rules","title":"Metric Rules","text":"<p>The <code>metricRules</code> section configures which JMX beans are exposed and how they are mapped to Prometheus metrics. The default configuration uses <code>pattern: \".*\"</code> to export all metrics, but production setups should restrict this to relevant Kafka Streams/producer/consumer metrics.</p> <p>Example rule set:</p> <pre><code>prometheus:\n  jmx:\n    metricRules:\n      - pattern: \"kafka.streams&lt;type=(.+), client-id=(.+), (.+)=(.+)&gt;&lt;&gt;(.+):\"\n        name: kafka_streams_$1_$5\n        labels:\n          client_id: \"$2\"\n          $3: \"$4\"\n      - pattern: \"kafka.producer&lt;type=(.+), client-id=(.+)&gt;&lt;&gt;(.+):\"\n        name: kafka_producer_$1_$3\n        labels:\n          client_id: \"$2\"\n</code></pre> <p>A ConfigMap containing the JMX exporter configuration is generated automatically by the chart and mounted into the sidecar container.</p>"},{"location":"user/monitoring/#prometheus-integration","title":"Prometheus Integration","text":""},{"location":"user/monitoring/#pod-annotations","title":"Pod annotations","text":"<p>For Prometheus instances that use pod annotations for discovery:</p> <pre><code>podAnnotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/path: \"/metrics\"\n  prometheus.io/port: \"5556\"\n</code></pre> <p>This enables scraping the JMX exporter endpoint exposed on <code>prometheus.jmx.port</code>.</p>"},{"location":"user/monitoring/#podmonitor","title":"PodMonitor","text":"<p>For more advanced Prometheus Operator setups, a <code>PodMonitor</code> custom resource can be deployed.</p> <p>The <code>streams-bootstrap</code> repository provides a reference <code>PodMonitor</code> configuration: monitoring/pod_monitor.yaml</p>"},{"location":"user/monitoring/#health-checks","title":"Health Checks","text":"<p>Kubernetes uses liveness and readiness probes to determine when a pod is healthy and when it is ready to receive traffic.</p> <p>Liveness probes restart containers that become unhealthy:</p> <pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n</code></pre> <p>Readiness probes gate traffic until the application is ready:</p> <pre><code>readinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 10\n  periodSeconds: 5\n  timeoutSeconds: 3\n  successThreshold: 1\n  failureThreshold: 3\n</code></pre> <p>All standard Kubernetes probe types are supported (<code>httpGet</code>, <code>tcpSocket</code>, <code>exec</code>, and <code>grpc</code> on recent Kubernetes versions). Probes are configured under the corresponding <code>livenessProbe</code> and <code>readinessProbe</code> sections in values.</p>"},{"location":"user/monitoring/#service-and-port-configuration","title":"Service and Port Configuration","text":"<p>Ports and Services control how HTTP APIs and metrics endpoints are exposed:</p> <pre><code>service:\n  enabled: true\n  type: ClusterIP\n\nports:\n  - containerPort: 8080\n    name: http\n    protocol: TCP\n    servicePort: 80\n  - containerPort: 5556\n    name: metrics\n    protocol: TCP\n    servicePort: 5556\n</code></pre> <p>Port mapping reference:</p> <ul> <li><code>jmx.port</code> \u2192 JMX remote port (default <code>5555</code>).</li> <li><code>prometheus.jmx.port</code> \u2192 JMX exporter metrics port (default <code>5556</code>).</li> <li>Additional entries in <code>ports[]</code> \u2192 application-specific ports (e.g. HTTP APIs, custom metrics endpoints).</li> </ul>"},{"location":"user/monitoring/#monitoring-configuration-examples","title":"Monitoring Configuration Examples","text":"<p>Full monitoring stack (JMX exporter, probes, Service, annotations):</p> <pre><code>prometheus:\n  jmx:\n    enabled: true\n    port: 5556\n    metricRules:\n      - pattern: \"kafka.streams&lt;type=stream-metrics, client-id=(.+)&gt;&lt;&gt;(.+):\"\n        name: kafka_streams_$2\n        labels:\n          client_id: \"$1\"\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nlivenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 60\n  periodSeconds: 30\n\nservice:\n  enabled: true\n  type: ClusterIP\n\nports:\n  - containerPort: 8080\n    name: http\n    servicePort: 80\n  - containerPort: 5556\n    name: metrics\n    servicePort: 5556\n\npodAnnotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/path: \"/metrics\"\n  prometheus.io/port: \"5556\"\n</code></pre> <p>Development/debug configuration with JMX only:</p> <pre><code>jmx:\n  enabled: true\n  port: 5555\n  host: localhost\n\nprometheus:\n  jmx:\n    enabled: false\n\nlivenessProbe:\n  tcpSocket:\n    port: 5555\n  initialDelaySeconds: 30\n  periodSeconds: 30\n</code></pre> <p>Minimal production configuration with annotations and resource limits:</p> <pre><code>prometheus:\n  jmx:\n    enabled: true\n    resources:\n      requests:\n        cpu: 10m\n        memory: 100Mi\n      limits:\n        cpu: 100m\n        memory: 100Mi\n\npodAnnotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/port: \"5556\"\n</code></pre>"},{"location":"user/monitoring/#application-specific-considerations","title":"Application-Specific Considerations","text":"<p>Kafka Streams applications expose metrics under several JMX domains, including <code>kafka.streams</code>, <code>kafka.producer</code>, and <code>kafka.consumer</code>. Commonly monitored metrics include:</p> <ul> <li><code>kafka.streams.state</code>: Overall application state (running, rebalancing, error).</li> <li><code>kafka.streams.commit-latency-avg</code>: Average commit latency.</li> <li><code>kafka.consumer.records-lag-max</code>: Maximum records lag per partition.</li> <li><code>kafka.producer.record-send-rate</code>: Producer throughput.</li> </ul> <p>Producer and consumer applications (via <code>producer-app</code> and <code>consumer-app</code> charts) use the same <code>prometheus.jmx</code> structure but may differ in availability patterns (for example, Jobs vs Deployments).</p>"},{"location":"user/monitoring/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues:</p> <ul> <li>No metrics endpoint:<ul> <li>Ensure <code>prometheus.jmx.enabled: true</code>.</li> </ul> </li> <li>Connection refused on JMX port:<ul> <li>Ensure <code>jmx.enabled: true</code> and the port is exposed.</li> </ul> </li> <li>Empty metrics response:<ul> <li>Review <code>metricRules</code> patterns; overly restrictive rules may filter out all metrics.</li> </ul> </li> <li>High exporter CPU usage:<ul> <li>Avoid <code>pattern: \".*\"</code> in production; use targeted patterns instead.</li> </ul> </li> <li>Pod not ready:<ul> <li>Validate liveness/readiness probe configuration and the corresponding application endpoints.</li> </ul> </li> </ul> <p>Verifying metrics export:</p> <pre><code>kubectl port-forward &lt;pod-name&gt; 5556:5556\ncurl http://localhost:5556/metrics\n</code></pre> <p>Debugging JMX connection:</p> <pre><code>kubectl port-forward &lt;pod-name&gt; 5555:5555\njconsole localhost:5555\n</code></pre> <p>If connection fails, verify that JMX is enabled, the port is mapped in <code>ports</code>, and the JVM has been started with the correct JMX system properties.</p>"},{"location":"user/testing/","title":"Testing","text":"<p>The <code>streams-bootstrap</code> testing tools provide utilities for testing Kafka Streams, Consumer, Producer and Consumer-Producer applications, covering both unit-level and integration-style scenarios.</p> <p>They abstract common test concerns such as Kafka infrastructure setup, Schema Registry integration, application lifecycle handling, and consumer group verification, and are designed to work with real Kafka clusters as well as schema-aware test environments.</p>"},{"location":"user/testing/#testapplicationrunner","title":"TestApplicationRunner","text":"<p><code>TestApplicationRunner</code> is a test utility for running, configuring, and verifying Kafka applications in integration and system tests.</p> <p>It abstracts away repetitive setup such as: - bootstrap servers - Schema Registry - Kafka client configuration - CLI argument wiring - lifecycle commands (<code>run</code>, <code>clean</code>, <code>reset</code>)</p> <p>Typical use cases: - end-to-end tests - containerized test environments - embedded Kafka setups - CI pipelines</p>"},{"location":"user/testing/#typical-usage","title":"Typical Usage","text":"<pre><code>TestApplicationRunner runner =\n        TestApplicationRunner.create(\"localhost:9092\")\n                .withSchemaRegistry()\n                .withStateDir(tempDir)\n                .withNoStateStoreCaching();\n</code></pre> <p>All applications executed via this runner automatically inherit this configuration.</p> <p>Bootstrap Servers - Passed via <code>--bootstrap-servers</code> - Also set directly on the application instance</p> <p>Kafka Configuration - All provided Kafka properties are injected - Passed via <code>--kafka-config key=value</code> - Also merged into <code>app.setKafkaConfig(...)</code></p> <p>Schema Registry (optional) - Passed via <code>--schema-registry-url</code> - Only configured when explicitly enabled</p>"},{"location":"user/testing/#configuring-kafka-for-tests","title":"Configuring Kafka for Tests","text":"<pre><code>runner = runner.withKafkaConfig(Map.of(\"auto.offset.reset\", \"earliest\"));\n</code></pre> <p>Behavior: - merged with existing configuration - immutable after creation - overrides application defaults</p>"},{"location":"user/testing/#kafka-streamsspecific-helpers","title":"Kafka Streams\u2013Specific Helpers","text":""},{"location":"user/testing/#configure-state-directory","title":"Configure State Directory","text":"<pre><code>runner = runner.withStateDir(tempDir);\n</code></pre> <p>Sets:</p> <pre><code>state.dir = &lt;tempDir&gt;\n</code></pre> <p>Use this to: - isolate test runs - avoid state leakage between tests</p>"},{"location":"user/testing/#disable-state-store-caching","title":"Disable State Store Caching","text":"<pre><code>runner = runner.withNoStateStoreCaching();\n</code></pre> <p>Sets:</p> <pre><code>statestore.cache.max.bytes = 0\n</code></pre> <p>Useful when: - asserting exact record counts - debugging processor behavior - avoiding cache-related timing issues</p>"},{"location":"user/testing/#consumer-specific-helpers","title":"Consumer-Specific Helpers","text":""},{"location":"user/testing/#configure-session-timeout","title":"Configure Session Timeout","text":"<pre><code>runner = runner.withSessionTimeout(Duration.ofSeconds(5));\n</code></pre> <p>Sets:</p> <pre><code>session.timeout.ms = 5000\n</code></pre> <p>Useful for: - fast consumer group rebalancing - deterministic failure testing</p>"},{"location":"user/testing/#schema-registry-support","title":"Schema Registry Support","text":""},{"location":"user/testing/#enable-a-test-schema-registry","title":"Enable a Test Schema Registry","text":"<pre><code>runner = runner.withSchemaRegistry();\n</code></pre> <p>Creates: - isolated in-memory Schema Registry - random scope to avoid collisions - transparent integration for the application</p>"},{"location":"user/testing/#use-a-custom-testschemaregistry","title":"Use a Custom TestSchemaRegistry","text":"<pre><code>TestSchemaRegistry registry = new TestSchemaRegistry();\nrunner = runner.withSchemaRegistry(registry);\n</code></pre> <p>Use this when: - sharing schemas across applications - inspecting registered schemas during tests</p>"},{"location":"user/testing/#running-applications","title":"Running Applications","text":""},{"location":"user/testing/#cli","title":"CLI","text":"<pre><code>CompletableFuture&lt;Integer&gt; exitCode =\n        runner.run(app, \"--some-flag\");\n</code></pre> <ul> <li>invokes <code>startApplicationWithoutExit</code></li> <li>returns application exit code</li> </ul>"},{"location":"user/testing/#runnable","title":"Runnable","text":"<pre><code>CompletableFuture&lt;Void&gt; execution = runner.run(app);\n</code></pre> <ul> <li>calls <code>onApplicationStart()</code></li> <li>runs application directly</li> <li>suitable for long-running tests</li> </ul>"},{"location":"user/testing/#cleaning-and-resetting-applications","title":"Cleaning and Resetting Applications","text":""},{"location":"user/testing/#clean","title":"Clean","text":"<pre><code>runner.clean(app);\n</code></pre> <p>or</p> <pre><code>runner.clean(app, \"--custom-arg\");\n</code></pre> <p>Used to: - delete Kafka topics - clean local state - execute cleanup hooks</p>"},{"location":"user/testing/#reset","title":"Reset","text":"<p>Supported for: - Streams applications - Consumer applications - Consumer\u2013Producer applications</p> <pre><code>runner.reset(streamsApp);\n</code></pre>"},{"location":"user/testing/#consumer-group-verification","title":"Consumer Group Verification","text":"<pre><code>ConsumerGroupVerifier verifier = runner.verify(streamsApp);\n</code></pre> <p>Allows you to: - assert consumer group existence - check stability - inspect committed offsets</p>"},{"location":"user/testing/#creating-test-clients","title":"Creating Test Clients","text":"<pre><code>KafkaTestClient client = runner.newTestClient();\n</code></pre> <p>Provides: - AdminClient access - Producer/Consumer helpers - runtime-aware configuration</p>"},{"location":"user/testing/#testapplicationtopologyfactory","title":"TestApplicationTopologyFactory","text":"<p><code>TestApplicationTopologyFactory</code> is a test helper for Kafka Streams applications that integrates with Fluent Kafka Streams Tests.</p> <p>It allows you to: - derive a <code>TestTopology</code> from a real application - reuse production topology and configuration - inject test-specific runtime settings</p>"},{"location":"user/testing/#typical-usage_1","title":"Typical Usage","text":"<pre><code>TestApplicationTopologyFactory factory =\n        TestApplicationTopologyFactory.withSchemaRegistry();\n</code></pre> <p>or without Schema Registry:</p> <pre><code>TestApplicationTopologyFactory factory = new TestApplicationTopologyFactory();\n</code></pre>"},{"location":"user/testing/#schema-registry-support_1","title":"Schema Registry Support","text":""},{"location":"user/testing/#automatic-schema-registry","title":"Automatic Schema Registry","text":"<pre><code>TestApplicationTopologyFactory factory =\n        TestApplicationTopologyFactory.withSchemaRegistry();\n</code></pre> <ul> <li>random isolated scope</li> <li>no cross-test collisions</li> <li>safe for parallel execution</li> </ul>"},{"location":"user/testing/#custom-schema-registry","title":"Custom Schema Registry","text":"<pre><code>TestSchemaRegistry registry = new TestSchemaRegistry();\nTestApplicationTopologyFactory factory =\n        TestApplicationTopologyFactory.withSchemaRegistry(registry);\n</code></pre>"},{"location":"user/testing/#modifying-kafka-configuration","title":"Modifying Kafka Configuration","text":"<pre><code>factory = factory.with(Map.of(\"commit.interval.ms\", 100));\n</code></pre> <ul> <li>merged into runtime configuration</li> <li>applies only to tests</li> <li>does not mutate application</li> </ul>"},{"location":"user/testing/#creating-a-testtopology","title":"Creating a TestTopology","text":"<pre><code>TestTopology&lt;String, MyValue&gt; topology = factory.createTopology(app);\n</code></pre> <p>Execution flow: 1. application prepared 2. runtime configuration injected 3. topology extracted 4. <code>TestTopology</code> created</p>"},{"location":"user/testing/#junit-5-integration","title":"JUnit 5 Integration","text":"<pre><code>TestTopologyExtension&lt;String, MyValue&gt; extension = factory.createTopologyExtension(app);\n</code></pre>"},{"location":"user/testing/#accessing-kafka-properties","title":"Accessing Kafka Properties","text":"<pre><code>Map&lt;String, Object&gt; props = factory.getKafkaProperties(app);\n</code></pre>"},{"location":"user/concepts/common/","title":"Common concepts","text":""},{"location":"user/concepts/common/#application-types","title":"Application types","text":"<p>In streams-bootstrap, there are three application types:</p> <ul> <li>App</li> <li>ConfiguredApp</li> <li>ExecutableApp</li> </ul>"},{"location":"user/concepts/common/#app","title":"App","text":"<p>The App represents your application logic implementation. Each application type has its own <code>App</code> interface:</p> <ul> <li>StreamsApp \u2013 for Kafka Streams applications</li> <li>ProducerApp \u2013 for producer applications</li> <li>ConsumerApp \u2013 for consumer applications</li> <li>ConsumerProducerApp \u2013 for consumer\u2013producer applications</li> </ul> <p>You implement the appropriate interface to define your application's behavior.</p>"},{"location":"user/concepts/common/#configuredapp","title":"ConfiguredApp","text":"<p>A ConfiguredApp pairs an <code>App</code> with its configuration. Examples include:</p> <ul> <li><code>ConfiguredConsumerApp&lt;T extends ConsumerApp&gt;</code></li> <li><code>ConfiguredProducerApp&lt;T extends ProducerApp&gt;</code></li> </ul> <p>This layer handles Kafka property creation, combining:</p> <ul> <li>base configuration</li> <li>app-specific configuration</li> <li>environment variables</li> <li>runtime configuration</li> </ul>"},{"location":"user/concepts/common/#executableapp","title":"ExecutableApp","text":"<p>An ExecutableApp is a <code>ConfiguredApp</code> with runtime configuration applied, making it ready to execute. It can create:</p> <ul> <li>a Runner for running the application</li> <li>a CleanUpRunner for cleanup operations</li> </ul>"},{"location":"user/concepts/common/#usage-pattern","title":"Usage Pattern","text":"<ol> <li>You implement an App.</li> <li>The system wraps it in a ConfiguredApp, applying the configuration.</li> <li> <p>Runtime configuration is then applied to create an ExecutableApp, which can be:</p> </li> <li> <p>run, or</p> </li> <li>cleaned up.</li> </ol>"},{"location":"user/concepts/common/#application-lifecycle","title":"Application lifecycle","text":"<p>Applications built with streams-bootstrap follow a defined lifecycle with specific states and transitions.</p> <p>The lifecycle is managed through the KafkaApplication base class and provides several extension points for customization.</p> Phase Description Entry Point Initialization Parse CLI arguments, inject environment variables, configure application <code>startApplication()</code> or <code>startApplicationWithoutExit()</code> Preparation Execute pre-run/pre-clean hooks <code>onApplicationStart()</code>, <code>prepareRun()</code>, <code>prepareClean()</code> Execution Run main application logic or cleanup operations <code>run()</code>, <code>clean()</code>, <code>reset()</code> Shutdown Stop runners, close resources, cleanup <code>stop()</code>, <code>close()</code>"},{"location":"user/concepts/common/#running-an-application","title":"Running an application","text":"<p>Applications built with streams-bootstrap can be started in two primary ways:</p> <ul> <li>Via Command Line Interface: When packaged as a runnable JAR (for example, in a container),   the <code>run</code> command is the default entrypoint. An example invocation:</li> </ul> <p><code>bash   java -jar example-app.jar \\       run \\       --bootstrap-servers kafka:9092 \\       --input-topics input-topic \\       --output-topic output-topic \\       --schema-registry-url http://schema-registry:8081</code></p> <ul> <li>Programmatically: The application subclass calls <code>startApplication(args)</code> on startup. Example for a Kafka Streams   application:</li> </ul> <pre><code>  public static void main(final String[] args) {\n    new MyStreamsApplication().startApplication(args);\n}\n</code></pre>"},{"location":"user/concepts/common/#cleaning-an-application","title":"Cleaning an application","text":"<p>A built-in mechanism is provided to clean up all resources associated with an application.</p> <p>When the cleanup operation is triggered, the following resources are removed:</p> Resource Type Description Streams Apps Producer Apps Consumer Apps Consumer-Producer Apps Output Topics The main output topic of the application \u2713 \u2713 N/A \u2713 Intermediate Topics Topics for stream operations like <code>through()</code> \u2713 N/A N/A N/A Internal Topics Topics for state stores or repartitioning (Kafka Streams) \u2713 N/A N/A N/A Consumer Groups Consumer group metadata \u2713 N/A \u2713 \u2713 Schema Registry All registered schemas \u2713 \u2713 \u2713 \u2713 <p>Cleanup can be triggered:</p> <ul> <li>Via Command Line: Helm cleanup jobs</li> <li>Programmatically:</li> </ul> <pre><code>// For streams applications\ntry(StreamsCleanUpRunner cleanUpRunner = streamsApp.createCleanUpRunner()){\n        cleanUpRunner.\n\nclean();\n}\n\n// For producer applications\n        try(\nCleanUpRunner cleanUpRunner = producerApp.createCleanUpRunner()){\n        cleanUpRunner.\n\nclean();\n}\n</code></pre> <p>Cleanup operations are idempotent, meaning they can be safely retried without causing additional issues.</p>"},{"location":"user/concepts/common/#configuration","title":"Configuration","text":"<p>Kafka properties are applied in the following order (later values override earlier ones):</p> <ol> <li>Base configuration</li> <li>App config from .createKafkaProperties()</li> <li>Environment variables (<code>KAFKA_</code>)</li> <li>Runtime args (--bootstrap-servers, etc.)</li> <li>Serialization config from ProducerApp.defaultSerializationConfig() or StreamsApp.defaultSerializationConfig()</li> <li>CLI overrides via --kafka-config</li> </ol> <p>Environment variables with the <code>APP_ prefix</code> (configurable via <code>ENV_PREFIX</code>) are automatically parsed. Environment variables are converted to CLI arguments:</p> <pre><code>APP_BOOTSTRAP_SERVERS       \u2192      --bootstrap-servers\nAPP_SCHEMA_REGISTRY_URL     \u2192      --schema-registry-url\nAPP_OUTPUT_TOPIC            \u2192      --output-topic\n</code></pre> <p>Additionally, Kafka-specific environment variables with the <code>KAFKA_</code> prefix are automatically added to the Kafka configuration.</p>"},{"location":"user/concepts/common/#schema-registry-integration","title":"Schema Registry integration","text":"<p>When the <code>--schema-registry-url</code> option is provided:</p> <ul> <li>Schemas are registered automatically during application startup</li> <li>Schema cleanup is handled as part of the <code>clean</code> command</li> <li>Schema evolution is fully supported</li> </ul>"},{"location":"user/concepts/common/#command-line-interface","title":"Command line interface","text":"<p>A unified command-line interface is provided for application configuration.</p>"},{"location":"user/concepts/common/#cli-commands","title":"CLI Commands","text":"<ul> <li><code>run</code>: Run the application</li> <li><code>clean</code>: Delete topics and consumer groups</li> <li><code>reset</code>: Reset internal state and offsets (for Streams apps)</li> </ul>"},{"location":"user/concepts/common/#common-cli-configuration-options","title":"Common CLI Configuration Options","text":"<ul> <li><code>--bootstrap-servers</code>: Kafka bootstrap servers (required)</li> <li><code>--schema-registry-url</code>: URL for Avro serialization</li> <li><code>--kafka-config</code>: Key-value Kafka configuration</li> </ul>"},{"location":"user/concepts/producer/","title":"Producer applications","text":"<p>Producer applications generate data and send it to Kafka topics. They can be used to produce messages from various sources, such as databases, files, or real-time events.</p> <p>streams-bootstrap provides a structured way to build producer applications with consistent configuration handling, command-line support, and lifecycle management.</p>"},{"location":"user/concepts/producer/#application-lifecycle","title":"Application lifecycle","text":""},{"location":"user/concepts/producer/#running-an-application","title":"Running an application","text":"<p>Producer applications are executed using the <code>ProducerRunner</code>, which runs the producer logic defined by the application.</p> <p>Unlike Kafka Streams applications, producer applications typically:</p> <ul> <li>Run to completion and terminate automatically, or</li> <li>Run continuously when implemented as long-lived services</li> </ul> <p>The execution model is fully controlled by the producer implementation and its runnable logic.</p>"},{"location":"user/concepts/producer/#cleaning-an-application","title":"Cleaning an application","text":"<p>Producer applications support a dedicated <code>clean</code> command.</p> <pre><code>java -jar my-producer-app.jar \\\n  --bootstrap-servers localhost:9092 \\\n  --output-topic my-topic \\\n  clean\n</code></pre> <p>The clean process can perform the following operations:</p> <ul> <li>Delete output topics</li> <li>Delete registered schemas from Schema Registry</li> <li>Execute custom cleanup hooks defined by the application</li> </ul> <p>Applications can register custom cleanup logic by overriding <code>setupCleanUp</code>.</p>"},{"location":"user/concepts/producer/#configuration","title":"Configuration","text":""},{"location":"user/concepts/producer/#serialization-configuration","title":"Serialization configuration","text":"<p>Producer applications define key and value serialization using the <code>defaultSerializationConfig()</code> method in their <code>ProducerApp</code> implementation.</p> <pre><code>\n@Override\npublic SerializerConfig defaultSerializationConfig() {\n    return new SerializerConfig(StringSerializer.class, SpecificAvroSerializer.class);\n}\n</code></pre>"},{"location":"user/concepts/producer/#kafka-properties","title":"Kafka properties","text":""},{"location":"user/concepts/producer/#base-configuration","title":"Base configuration","text":"<p>The following Kafka properties are configured by default for Producer applications in streams-bootstrap:</p> <ul> <li><code>max.in.flight.requests.per.connection = 1</code></li> <li><code>acks = all</code></li> <li><code>compression.type = gzip</code> </li> </ul>"},{"location":"user/concepts/producer/#custom-kafka-properties","title":"Custom Kafka properties","text":"<p>Kafka configuration can be customized by overriding <code>createKafkaProperties()</code>:</p> <pre><code>@Override\npublic Map&lt;String, Object&gt; createKafkaProperties() {\n    return Map.of(\n            ProducerConfig.RETRIES_CONFIG, 3,\n            ProducerConfig.BATCH_SIZE_CONFIG, 16384,\n            ProducerConfig.LINGER_MS_CONFIG, 5\n    );\n}\n</code></pre> <p>These properties are merged with defaults and CLI-provided configuration.</p>"},{"location":"user/concepts/producer/#lifecycle-hooks","title":"Lifecycle hooks","text":"<p>Producer applications can register cleanup logic via <code>setupCleanUp</code>. This method allows you to attach:</p> <ul> <li>Cleanup hooks \u2013 for general cleanup logic not tied to Kafka topics</li> <li>Topic hooks \u2013 for reacting to topic lifecycle events (e.g. deletion)</li> </ul>"},{"location":"user/concepts/producer/#clean-up","title":"Clean up","text":"<p>Custom cleanup logic that is not tied to Kafka topics can be registered via cleanup hooks:</p> <pre><code>\n@Override\npublic ProducerCleanUpConfiguration setupCleanUp(\n        final AppConfiguration&lt;ProducerTopicConfig&gt; configuration) {\n\n    return ProducerApp.super.setupCleanUp(configuration)\n            .registerCleanHook(() -&gt; {\n                // Custom cleanup logic\n            });\n}\n</code></pre>"},{"location":"user/concepts/producer/#topic-hooks","title":"Topic hooks","text":"<p>Topic hooks should be used for topic-related cleanup or side effects, such as releasing external resources associated with a topic or logging topic deletions:</p> <pre><code>@Override\npublic ProducerCleanUpConfiguration setupCleanUp(\n        final AppConfiguration&lt;ProducerTopicConfig&gt; configuration) {\n\n    return ProducerApp.super.setupCleanUp(configuration)\n            .registerTopicHook(new TopicHook() {\n\n                @Override\n                public void deleted(final String topic) {\n                    // Called when a managed topic is deleted\n                    System.out.println(\"Deleted topic: \" + topic);\n                }\n\n                @Override\n                public void close() {\n                    // Optional cleanup for the hook itself\n                }\n            });\n}\n</code></pre>"},{"location":"user/concepts/producer/#command-line-interface","title":"Command line interface","text":"<p>Producer applications inherit standard CLI options from <code>KafkaApplication</code>. The following CLI options are producer-specific:</p> Option Description Default <code>--output-topic</code> Default output topic - <code>--labeled-output-topics</code> Named output topics (<code>label1=topic1,...</code>) -"},{"location":"user/concepts/producer/#deployment","title":"Deployment","text":"<p>TODO</p>"},{"location":"user/concepts/streams/","title":"Streams applications","text":"<p>Streams apps are applications that process data in real-time as it flows through Kafka topics. They can be used to filter, transform, aggregate, or enrich data streams. Streams apps can also produce new messages to other topics based on the processed data.</p>"},{"location":"user/concepts/streams/#application-lifecycle","title":"Application lifecycle","text":""},{"location":"user/concepts/streams/#running-an-application","title":"Running an application","text":"<p>Kafka Streams applications are started via the <code>KafkaStreamsApplication</code> entry point:</p> <pre><code>public static void main(final String[] args) {\n    new MyStreamsApplication().startApplication(args);\n}\n</code></pre> <p>When an application is started, the following steps are performed:</p> <ul> <li>Parse command-line arguments and environment variables</li> <li>Create a <code>StreamsApp</code> instance</li> <li>Wrap it in a <code>ConfiguredStreamsApp</code></li> <li>Convert it into an <code>ExecutableStreamsApp</code></li> <li>Start execution using the <code>StreamsRunner</code></li> </ul>"},{"location":"user/concepts/streams/#resetting-an-application","title":"Resetting an application","text":"<p>Streams applications support a dedicated <code>reset</code> operation that clears processing state while preserving the application definition and configuration. This is useful for reprocessing input data from the beginning.</p> <p>When a reset is triggered, the following resources are affected:</p> Resource Action State stores Cleared locally, changelog topics deleted Internal topics Deleted (e.g. repartition topics) Consumer offsets Reset to earliest for input topics Output topics Preserved <p>Triggering a reset via CLI:</p> <pre><code>java -jar my-streams-app.jar reset\n</code></pre> <p>Triggering a reset programmatically:</p> <pre><code>try(StreamsCleanUpRunner cleanUpRunner = streamsApp.createCleanUpRunner()){\n        cleanUpRunner.\n\nreset();\n}\n</code></pre> <p>After a reset, the application can be started again and will reprocess all input data.</p>"},{"location":"user/concepts/streams/#cleaning-an-application","title":"Cleaning an application","text":"<p>The <code>clean</code> command performs everything that <code>reset</code> does and additionally removes the Kafka consumer groups and output topics created by the application.</p> <pre><code>java -jar my-streams-app.jar clean\n</code></pre>"},{"location":"user/concepts/streams/#configuration","title":"Configuration","text":""},{"location":"user/concepts/streams/#topics","title":"Topics","text":"<p>Streams applications support flexible topic configuration:</p> <ul> <li><code>--input-topics</code>: Comma-separated list of input topics</li> <li><code>--input-pattern</code>: Regex pattern for input topics</li> <li><code>--output-topic</code>: Default output topic</li> <li><code>--error-topic</code>: Topic for error records</li> <li><code>--labeled-input-topics</code>: Named input topics with different message types</li> <li><code>--labeled-input-patterns</code>: Additional labeled input topic patterns</li> <li><code>--labeled-output-topics</code>: Named output topics with different message types</li> </ul>"},{"location":"user/concepts/streams/#application-id","title":"Application ID","text":"<ul> <li><code>--application-id</code>: Unique Kafka Streams application ID</li> </ul>"},{"location":"user/concepts/streams/#kafka-properties","title":"Kafka properties","text":""},{"location":"user/concepts/streams/#base-configuration","title":"Base configuration","text":"<p>The following Kafka properties are configured by default for Streams applications in streams-bootstrap:</p> <ul> <li><code>processing.guarantee = exactly_once_v2</code></li> <li><code>producer.max.in.flight.requests.per.connection = 1</code></li> <li><code>producer.acks = all</code></li> <li><code>producer.compression.type = gzip</code></li> </ul>"},{"location":"user/concepts/streams/#custom-kafka-properties","title":"Custom Kafka properties","text":"<p>Kafka configuration can be customized by overriding <code>createKafkaProperties()</code>:</p> <pre><code>@Override\npublic Map&lt;String, Object&gt; createKafkaProperties() {\n    return Map.of(\n            StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE_V2,\n            StreamsConfig.NUM_STREAM_THREADS_CONFIG, 4,\n            StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, LogAndContinueExceptionHandler.class.getName()\n    );\n}\n</code></pre>"},{"location":"user/concepts/streams/#lifecycle-hooks","title":"Lifecycle hooks","text":"<p>Streams applications support the following hook types:</p> <ul> <li>Cleanup hooks \u2013 for general cleanup logic not tied to Kafka topics</li> <li>Topic hooks \u2013 for reacting to topic lifecycle events (e.g. deletion)</li> <li>Reset hooks \u2013 for logic that should run only during an application reset</li> <li></li> </ul>"},{"location":"user/concepts/streams/#setup","title":"Setup","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#clean-up","title":"Clean up","text":"<p>Use cleanup hooks for logic that is not tied to Kafka topics, such as closing external resources or cleaning up temporary state.</p> <pre><code>@Override\npublic StreamsCleanUpConfiguration setupCleanUp(\n        final AppConfiguration&lt;StreamsTopicConfig&gt; configuration) {\n\n    return StreamsApp.super.setupCleanUp(configuration)\n            .registerCleanHook(() -&gt; {\n                // Custom cleanup logic\n            });\n}\n</code></pre>"},{"location":"user/concepts/streams/#topic-hooks","title":"Topic hooks","text":"<p>Topic hooks allow Kafka Streams applications to react to Kafka topic lifecycle events, such as topic deletion during <code>clean</code> or <code>reset</code> operations.</p> <pre><code>@Override\npublic StreamsCleanUpConfiguration setupCleanUp(\n        final AppConfiguration&lt;StreamsTopicConfig&gt; configuration) {\n\n    return StreamsApp.super.setupCleanUp(configuration)\n            .registerTopicHook(new TopicHook() {\n                @Override\n                public void deleted(final String topic) {\n                    // Called when a managed topic is deleted\n                    System.out.println(\"Deleted topic: \" + topic);\n                }\n                @Override\n                public void close() {\n                    // Optional cleanup for the hook itself\n                }\n            });\n}\n</code></pre>"},{"location":"user/concepts/streams/#reset-hooks","title":"Reset hooks","text":"<p>Reset hooks allow Kafka Streams applications to execute custom logic only during a reset operation. They are not invoked during a regular clean.</p>"},{"location":"user/concepts/streams/#example-reset-hook-registration-streams","title":"Example: reset hook registration (Streams)","text":"<pre><code>@Override\npublic StreamsCleanUpConfiguration setupCleanUp(\n        final AppConfiguration&lt;StreamsTopicConfig&gt; configuration) {\n\n    return StreamsApp.super.setupCleanUp(configuration)\n            .registerResetHook(() -&gt; {\n                // Custom logic executed only during reset\n            });\n}\n</code></pre>"},{"location":"user/concepts/streams/#execution-options","title":"Execution options","text":""},{"location":"user/concepts/streams/#on-start","title":"On start","text":"<p>Custom logic can be executed once Kafka Streams has fully started:</p> <pre><code>\n@Override\nprivate void onStreamsStart(final RunningStreams runningStreams) {\n    // Custom startup logic\n}\n</code></pre>"},{"location":"user/concepts/streams/#application-server","title":"Application server","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#state-listener","title":"State listener","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#uncaught-exception-handler","title":"Uncaught exception handler","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#closing-options","title":"Closing options","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#command-line-interface","title":"Command line interface","text":"<p>Streams applications inherit standard CLI options from <code>KafkaStreamsApplication</code>. The following CLI options are streams-app-specific:</p> Option Description Default <code>--application-id</code> Kafka Streams application ID Auto-generated <code>--volatile-group-instance-id</code> Use volatile group instance ID false"},{"location":"user/concepts/streams/#deployment","title":"Deployment","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#kafka-streams-extensions","title":"Kafka Streams extensions","text":"<p>Several extensions are provided that simplify working with Kafka Streams.</p>"},{"location":"user/concepts/streams/#simple-topic-access","title":"Simple topic access","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#error-handling","title":"Error handling","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#serde-auto-configuration","title":"Serde auto configuration","text":"<p>TODO</p>"},{"location":"user/deployment/kubernetes/","title":"Deployment to Kubernetes","text":"<p><code>streams-bootstrap</code> provides support for deploying applications to Kubernetes using Helm charts. The charts cover Kafka Streams, producer, consumer, and producer-consumer applications and offer standardized solutions for autoscaling, monitoring, and state persistence.</p>"},{"location":"user/deployment/kubernetes/#core-capabilities","title":"Core capabilities","text":"<ul> <li>Autoscaling \u2013 KEDA-based horizontal scaling driven by Kafka consumer lag</li> <li>Monitoring \u2013 JMX metrics export with Prometheus integration</li> <li>Persistence \u2013 Persistent volumes for Kafka Streams state stores</li> </ul>"},{"location":"user/deployment/kubernetes/#helm-charts","title":"Helm charts","text":"<p>A set of Helm charts is shipped, tailored to different application types:</p> Chart name Purpose Kubernetes workload types <code>streams-app</code> Deploy Kafka Streams applications <code>Deployment</code>, <code>StatefulSet</code> <code>producer-app</code> Deploy Kafka Producer applications <code>Deployment</code>, <code>Job</code>, <code>CronJob</code> <code>consumer-app</code> Deploy Kafka Consumer applications <code>Deployment</code>, <code>StatefulSet</code> <code>consumerproducer-app</code> Deploy batch / consumer\u2013producer applications <code>Deployment</code>, <code>StatefulSet</code> <code>*-cleanup-job</code> Clean Kafka resources before deployment <code>Job</code> (Helm hooks)"},{"location":"user/deployment/kubernetes/#chart-repository-and-installation","title":"Chart repository and installation","text":"<p>The Helm charts are published as a Helm repository:</p> <pre><code>helm repo add streams-bootstrap https://bakdata.github.io/streams-bootstrap/\nhelm repo update\n</code></pre> <p>A Streams application can then be installed with:</p> <pre><code>helm install my-app bakdata-common/streams-app --values my-values.yaml\n</code></pre>"},{"location":"user/deployment/kubernetes/#deployment-patterns","title":"Deployment patterns","text":""},{"location":"user/deployment/kubernetes/#streams-consumer-and-consumerproducer-applications","title":"Streams, consumer and consumer\u2013producer applications","text":"<p>Streams, consumer and consumer\u2013producer applications support both stateless and stateful deployment modes:</p> <ul> <li> <p>Deployment</p> <ul> <li>Used for stateless applications or when state is stored externally</li> <li>Enabled when <code>statefulSet: false</code> or <code>persistence.enabled: false</code></li> </ul> </li> <li> <p>StatefulSet</p> <ul> <li>Used for stateful Kafka Streams applications with local state stores</li> <li>Enabled when <code>statefulSet: true</code></li> <li>Required when <code>persistence.enabled: true</code></li> <li>Each pod receives a dedicated <code>PersistentVolumeClaim</code> for RocksDB state</li> </ul> </li> </ul>"},{"location":"user/deployment/kubernetes/#producer-applications","title":"Producer applications","text":"<p>Producer applications support multiple execution modes depending on workload characteristics:</p> <ul> <li>Deployment</li> <li>Used for long-running or continuous producers</li> <li>Enabled when <code>deployment: true</code></li> <li> <p>Supports horizontal scaling via <code>replicaCount</code></p> </li> <li> <p>Job</p> </li> <li>Used for one-time runs or backfills</li> <li>Default when <code>deployment: false</code> and no <code>schedule</code> is provided</li> <li> <p>Supports <code>restartPolicy</code>, <code>backoffLimit</code>, and <code>ttlSecondsAfterFinished</code></p> </li> <li> <p>CronJob</p> </li> <li>Used for scheduled, periodic execution</li> <li>Enabled when a cron expression is provided via <code>schedule</code></li> <li>Supports <code>suspend</code>, <code>successfulJobsHistoryLimit</code>, and <code>failedJobsHistoryLimit</code></li> </ul>"},{"location":"user/deployment/kubernetes/#cleanup-jobs","title":"Cleanup jobs","text":"<p>Cleanup charts are executed as Helm hook Jobs:</p> <ul> <li>Run as <code>pre-install</code> or <code>pre-upgrade</code> hooks</li> <li>Remove:<ul> <li>Kafka topics</li> <li>Consumer groups</li> <li>Schema Registry subjects</li> </ul> </li> </ul> <p>This ensures a clean starting point for reprocessing or redeployment scenarios.</p>"},{"location":"user/deployment/kubernetes/#configuration-structure","title":"Configuration structure","text":"<p>TODO</p>"},{"location":"user/deployment/kubernetes/#environment-variable-mapping","title":"Environment variable mapping","text":"<p>Helm values are translated into environment variables using a configurable prefix:</p> <pre><code>configurationEnvPrefix: \"APP\"\n\ncommandLine:\n  MY_PARAM: \"value\"\nkafka:\n  inputTopics: [ \"input\" ]\n  outputTopic: \"output\"\n</code></pre> <p>This results in:</p> <ul> <li><code>APP_MY_PARAM=value</code></li> <li><code>APP_INPUT_TOPICS=input</code></li> <li><code>APP_OUTPUT_TOPIC=output</code></li> </ul> <p>Kafka client configuration uses the <code>KAFKA_</code> prefix:</p> <pre><code>kafka:\n  config:\n    max.poll.records: 500\n</code></pre> <p>Becomes:</p> <ul> <li><code>KAFKA_MAX_POLL_RECORDS=500</code></li> </ul>"},{"location":"user/deployment/kubernetes/#autoscaling","title":"Autoscaling","text":"<p>Autoscaling is implemented using Kubernetes Event-Driven Autoscaling (KEDA). When enabled, KEDA monitors Kafka consumer lag and adjusts the number of replicas accordingly.</p> <p>Autoscaling is disabled by default.</p>"},{"location":"user/deployment/kubernetes/#enabling-autoscaling","title":"Enabling autoscaling","text":"<pre><code>autoscaling:\n  enabled: true\n  lagThreshold: \"1000\"\n  minReplicas: 0\n  maxReplicas: 5\n</code></pre> <p>When enabled, the chart creates a KEDA <code>ScaledObject</code> and omits a fixed <code>replicaCount</code> from the workload specification.</p>"},{"location":"user/deployment/kubernetes/#scaling-behavior","title":"Scaling behavior","text":"<p>KEDA computes the desired number of replicas as:</p> <pre><code>desiredReplicas = ceil(totalLag / lagThreshold)\n</code></pre> <p>subject to <code>minReplicas</code> and <code>maxReplicas</code>.</p>"},{"location":"user/deployment/kubernetes/#integration-with-persistence","title":"Integration with persistence","text":"<p>When persistence is enabled for Streams applications, autoscaling targets a <code>StatefulSet</code>. Each replica receives its own <code>PersistentVolumeClaim</code>.</p> <p>Note: Scale-down operations remove pods and their PVCs. Backup and recovery strategies should be considered.</p>"},{"location":"user/deployment/kubernetes/#monitoring","title":"Monitoring","text":"<p>Monitoring is based on JMX metrics and Prometheus scraping:</p> <ul> <li><code>jmx.enabled: true</code> enables Kafka client and Streams metrics</li> <li><code>prometheus.jmx.enabled: true</code> adds a Prometheus JMX exporter sidecar</li> <li>Metrics are exposed on a dedicated <code>/metrics</code> endpoint</li> </ul> <p>Collected metrics include consumer lag, processing rates, and RocksDB statistics.</p>"},{"location":"user/deployment/kubernetes/#persistence","title":"Persistence","text":"<p>Persistence is configured via the <code>persistence.*</code> section (Streams applications only):</p> <pre><code>persistence:\n  enabled: true\n  size: 1Gi\n  storageClassName: standard\n</code></pre> <p>When enabled together with <code>statefulSet: true</code>, each pod receives a dedicated volume for local state storage. This enables:</p> <ul> <li>Faster restarts due to warm state</li> <li>Improved recovery semantics for stateful topologies</li> </ul> <p>If persistence is disabled, applications behave as stateless deployments and rely on Kafka changelogs for state reconstruction.</p>"},{"location":"user/deployment/local/","title":"Local deployment","text":"<p>Applications can be run locally for development and testing purposes. This can be done programmatically within your code.</p>"},{"location":"user/deployment/local/#programmatic-local-execution","title":"Programmatic Local Execution","text":"<p>Here is an example of how to run a producer application programmatically. This is useful for simple applications or for testing.</p> <pre><code>try(final KafkaProducerApplication&lt;?&gt; app = new SimpleKafkaProducerApplication&lt;&gt;(() -&gt;\n        new ProducerApp() {\n            @Override\n            public ProducerRunnable buildRunnable(final ProducerBuilder builder) {\n                return () -&gt; {\n                    try (final Producer&lt;Object, Object&gt; producer = builder.createProducer()) {\n                        // Producer logic\n                    }\n                };\n            }\n\n            @Override\n            public SerializerConfig defaultSerializationConfig() {\n                return new SerializerConfig(StringSerializer.class, StringSerializer.class);\n            }\n        }\n)){\n        app.\n\nsetBootstrapServers(\"localhost:9092\");\n    app.\n\nsetOutputTopic(\"output-topic\");\n    app.\n\nrun();\n}\n</code></pre>"},{"location":"user/deployment/local/#command-line-execution","title":"Command Line Execution","text":"<p>You can also run the application from the command line by packaging it as a JAR file.</p> <pre><code>java -jar my-producer-app.jar --bootstrap-servers localhost:9092 --output-topic my-topic run\n</code></pre>"},{"location":"user/examples/interactive-queries/","title":"Interactive queries","text":""},{"location":"user/examples/word-count/","title":"Word count","text":""},{"location":"user/extensions/large-messages/","title":"Large messages","text":""},{"location":"user/extensions/large-messages/#overview","title":"Overview","text":"<p>The Large Messages extension adds support for handling messages that exceed Kafka's size limitations by using external storage mechanisms with automatic cleanup. It integrates with streams-bootstrap to transparently manage:</p> <ul> <li>large message serialization</li> <li>large message deserialization</li> <li>blob storage files cleanup</li> </ul> <p>For more details, see the large messages module: streams-bootstrap-large-messages GitHub repository</p> <p>There are two supported ways to enable cleanup for large messages:</p> <ul> <li>Implement <code>LargeMessageStreamsApp</code></li> <li>Register a topic cleanup hook manually</li> </ul>"},{"location":"user/extensions/large-messages/#option-1-implement-largemessagestreamsapp","title":"Option 1: Implement <code>LargeMessageStreamsApp</code>","text":"<p>Use this option for Kafka Streams applications where large message cleanup should always run together with topic cleanup.</p> <pre><code>public final class MyStreamsApp implements LargeMessageStreamsApp {\n\n    @Override\n    public void buildTopology(final StreamsBuilderX builder) {\n        // build topology here\n    }\n}\n</code></pre>"},{"location":"user/extensions/large-messages/#option-2-register-a-cleanup-hook-manually","title":"Option 2: Register a cleanup hook manually","text":"<p>If cleanup should only happen conditionally or requires custom behavior, a topic hook can be registered explicitly.</p> <pre><code>private final boolean largeMessageCleanupEnabled;\n\n@Override\npublic StreamsCleanUpConfiguration setupCleanUp(\n        final AppConfiguration&lt;StreamsTopicConfig&gt; configuration) {\n\n    final StreamsCleanUpConfiguration cleanUp =\n            StreamsApp.super.setupCleanUp(configuration);\n\n    if (this.largeMessageCleanupEnabled) {\n        LargeMessageAppUtils.registerTopicHook(cleanUp, configuration);\n    }\n\n    return cleanUp;\n}\n</code></pre>"},{"location":"user/getting-started/quick-start/","title":"Quick Start","text":"<p>This page shows how to add <code>streams-bootstrap</code> to a project and how to create and run a minimal application.</p>"},{"location":"user/getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Java 17</li> <li>Apache Kafka cluster (brokers reachable from the application)</li> <li><code>streams-bootstrap-cli</code> dependency (see Setup for Gradle/Maven snippets)</li> </ul>"},{"location":"user/getting-started/quick-start/#minimal-kafka-streams-application","title":"Minimal Kafka Streams Application","text":"<p>Create a subclass of <code>KafkaStreamsApplication</code> and implement the required methods.</p> <pre><code>import com.bakdata.kafka.streams.KafkaStreamsApplication;\nimport com.bakdata.kafka.streams.SerdeConfig;\nimport com.bakdata.kafka.streams.StreamsApp;\nimport com.bakdata.kafka.streams.StreamsTopicConfig;\nimport com.bakdata.kafka.streams.kstream.KStreamX;\nimport com.bakdata.kafka.streams.kstream.StreamsBuilderX;\nimport java.util.Map;\nimport org.apache.kafka.common.serialization.Serdes.StringSerde;\n\npublic class MyStreamsApplication extends KafkaStreamsApplication&lt;StreamsApp&gt; {\n\n    public static void main(final String[] args) {\n        new MyStreamsApplication().startApplication(args);\n    }\n\n    @Override\n    public StreamsApp createApp() {\n        return new StreamsApp() {\n            @Override\n            public void buildTopology(final StreamsBuilderX builder) {\n                final KStreamX&lt;String, String&gt; input = builder.streamInput();\n                // topology definition\n                input.toOutputTopic();\n            }\n\n            @Override\n            public String getUniqueAppId(final StreamsTopicConfig topics) {\n                return \"streams-bootstrap-app-\" + topics.getOutputTopic();\n            }\n\n            @Override\n            public SerdeConfig defaultSerializationConfig() {\n                return new SerdeConfig(StringSerde.class, StringSerde.class);\n            }\n        };\n    }\n}\n</code></pre>"},{"location":"user/getting-started/quick-start/#running-the-application","title":"Running the Application","text":""},{"location":"user/getting-started/quick-start/#via-command-line-interface","title":"Via Command Line Interface","text":"<p>When packaged as a runnable JAR (for example, in a container), the <code>run</code> command is the default entrypoint:</p> <pre><code>java -jar my-streams-app.jar \\\n    run \\\n    --bootstrap-servers kafka:9092 \\\n    --input-topics input-topic \\\n    --output-topic output-topic \\\n    --schema-registry-url http://schema-registry:8081\n</code></pre> <p>Additional subcommands such as <code>clean</code> and <code>reset</code> are available for lifecycle management.</p>"},{"location":"user/getting-started/quick-start/#from-the-main-method","title":"From the <code>main</code> Method","text":"<p>In the <code>main</code> method, the application subclass starts up via:</p> <pre><code>public static void main(final String[] args) {\n    new MyStreamsApplication().startApplication(args);\n}\n</code></pre> <p>This delegates configuration loading, lifecycle handling, and shutdown to <code>streams-bootstrap</code>.</p>"},{"location":"user/getting-started/setup/","title":"Setup","text":"<p>This page describes dependency setup, configuration options, commands, and Helm-based deployment for <code>streams-bootstrap</code>.</p>"},{"location":"user/getting-started/setup/#dependencies","title":"Dependencies","text":""},{"location":"user/getting-started/setup/#gradle","title":"Gradle","text":"<pre><code>implementation group: 'com.bakdata.kafka', name: 'streams-bootstrap-cli', version: '6.1.0'\n</code></pre> <p>With Kotlin DSL:</p> <pre><code>implementation(group = \"com.bakdata.kafka\", name = \"streams-bootstrap-cli\", version = \"6.1.0\")\n</code></pre>"},{"location":"user/getting-started/setup/#maven","title":"Maven","text":"<pre><code>\n&lt;dependency&gt;\n    &lt;groupId&gt;com.bakdata.kafka&lt;/groupId&gt;\n    &lt;artifactId&gt;streams-bootstrap-cli&lt;/artifactId&gt;\n    &lt;version&gt;6.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>For other build tools or versions, refer to the latest version in MvnRepository.</p>"}]}