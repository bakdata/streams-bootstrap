{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"What is streams-bootstrap?","text":"<p><code>streams-bootstrap</code> is a Java library that standardizes the development and operation of Kafka-based applications (Kafka Streams and plain Kafka clients).</p> <p>The framework supports Apache Kafka 4.1 and Java 17. Its modules are published to Maven Central for straightforward integration into existing projects.</p>"},{"location":"#why-use-it","title":"Why use it?","text":"<p>Kafka Streams and the core Kafka clients provide strong primitives for stream processing and messaging, but they do not prescribe:</p> <ul> <li>How to structure a full application around those primitives</li> <li>How to configure applications consistently</li> <li>How to deploy and operate these services on Kubernetes</li> <li>How to perform repeatable reprocessing and cleanup</li> <li>How to handle errors and large messages uniformly</li> </ul> <p><code>streams-bootstrap</code> addresses these aspects by supplying:</p> <ol> <li>Standardized base classes for Kafka Streams and client applications.</li> <li>A common CLI/configuration contract for all Kafka applications.</li> <li>Helm-based deployment templates and conventions for Kubernetes.</li> <li>Built-in reset/clean workflows for reprocessing and state management.</li> <li>Consistent error-handling and dead-letter integration.</li> <li>Testing infrastructure for local development and CI environments.</li> <li>Optional blob-storage-backed serialization for large messages.</li> </ol>"},{"location":"#framework-architecture","title":"Framework Architecture","text":"<p>The framework uses a modular architecture with a clear separation of concerns.</p>"},{"location":"#core-modules","title":"Core Modules","text":"<ul> <li><code>streams-bootstrap-core</code>: Base classes such as <code>KafkaApplication</code>, <code>Runner</code>, and <code>CleanUpRunner</code></li> <li><code>streams-bootstrap-cli</code>: CLI framework based on <code>picocli</code></li> <li><code>streams-bootstrap-test</code>: Testing utilities (<code>TestApplicationRunner</code>, <code>KafkaTestClient</code>)</li> <li><code>streams-bootstrap-large-messages</code>: Support for handling large Kafka messages</li> <li><code>streams-bootstrap-cli-test</code>: Test support for CLI-based applications</li> </ul>"},{"location":"#external-dependencies","title":"External Dependencies","text":"<ul> <li>Apache Kafka: <code>kafka-streams</code>, <code>kafka-clients</code></li> <li>Confluent Platform: Schema Registry and Avro SerDes</li> <li>Picocli: Command-line parsing and CLI framework</li> </ul>"},{"location":"javadoc/legal/jquery/","title":"Jquery","text":""},{"location":"javadoc/legal/jquery/#jquery-v361","title":"jQuery v3.6.1","text":""},{"location":"javadoc/legal/jquery/#jquery-license","title":"jQuery License","text":"<pre><code>jQuery v 3.6.1\nCopyright OpenJS Foundation and other contributors, https://openjsf.org/\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n******************************************\n\nThe jQuery JavaScript Library v3.6.1 also includes Sizzle.js\n\nSizzle.js includes the following license:\n\nCopyright JS Foundation and other contributors, https://js.foundation/\n\nThis software consists of voluntary contributions made by many\nindividuals. For exact contribution history, see the revision history\navailable at https://github.com/jquery/sizzle\n\nThe following license applies to all parts of this software except as\ndocumented below:\n\n====\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n====\n\nAll files located in the node_modules and external directories are\nexternally maintained libraries used by this software which have their\nown licenses; we recommend you read them, as their terms may differ from\nthe terms above.\n\n*********************\n\n</code></pre>"},{"location":"javadoc/legal/jqueryUI/","title":"jqueryUI","text":""},{"location":"javadoc/legal/jqueryUI/#jquery-ui-v1132","title":"jQuery UI v1.13.2","text":""},{"location":"javadoc/legal/jqueryUI/#jquery-ui-license","title":"jQuery UI License","text":"<pre><code>Copyright jQuery Foundation and other contributors, https://jquery.org/\n\nThis software consists of voluntary contributions made by many\nindividuals. For exact contribution history, see the revision history\navailable at https://github.com/jquery/jquery-ui\n\nThe following license applies to all parts of this software except as\ndocumented below:\n\n====\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n====\n\nCopyright and related rights for sample code are waived via CC0. Sample\ncode is defined as all source code contained within the demos directory.\n\nCC0: http://creativecommons.org/publicdomain/zero/1.0/\n\n====\n\nAll files located in the node_modules and external directories are\nexternally maintained libraries used by this software which have their\nown licenses; we recommend you read them, as their terms may differ from\nthe terms above.\n\n</code></pre>"},{"location":"user/monitoring/","title":"Monitoring","text":"<p>The framework provides features for monitoring your applications.</p> <ul> <li>JMX Metrics Export: Applications built with <code>streams-bootstrap</code> can expose JMX (Java Management Extensions)   metrics, which provide insights into the performance and health of the Java application and the Kafka clients.</li> <li>Prometheus Integration: The Helm charts are configured to work with Prometheus, a popular open-source monitoring   and alerting toolkit. This allows you to scrape the JMX metrics and visualize them in dashboards (e.g., using   Grafana).</li> </ul>"},{"location":"user/monitoring/#monitoring-and-observability","title":"Monitoring and Observability","text":"<p>The Helm charts provide integrated monitoring and observability for Kafka applications using a combination of JMX, Prometheus, Kubernetes probes, and Services. Monitoring can be tailored from lightweight setups for development to full production stacks with Prometheus Operator.</p>"},{"location":"user/monitoring/#monitoring-mechanisms","title":"Monitoring Mechanisms","text":"Mechanism Use Case Key Values JMX remote access Direct debugging and inspection <code>jmx.enabled</code> Prometheus JMX exporter Production metrics collection <code>prometheus.jmx.enabled</code> Liveness probes Container health checks <code>livenessProbe</code> Readiness probes Traffic readiness / rollout control <code>readinessProbe</code> Custom ports and Service HTTP / metrics endpoints exposure <code>ports</code>, <code>service.*</code>"},{"location":"user/monitoring/#jmx-configuration","title":"JMX Configuration","text":"<p>JMX (Java Management Extensions) provides direct access to application metrics and management operations, typically used for development and debugging.</p> <p>Enable JMX in <code>values.yaml</code>:</p> <pre><code>jmx:\n  enabled: true\n  port: 5555\n  host: localhost\n</code></pre> <p>Parameters:</p> <ul> <li><code>jmx.enabled</code>: Enable JMX port for remote access (default: <code>false</code>).</li> <li><code>jmx.port</code>: JMX port number (default: <code>5555</code>).</li> <li><code>jmx.host</code>: Host binding for the RMI server (default: <code>localhost</code>).</li> </ul> <p>When enabled, the chart configures the JVM with flags similar to:</p> <pre><code>-Dcom.sun.management.jmxremote\n-Dcom.sun.management.jmxremote.port=5555\n-Dcom.sun.management.jmxremote.local.only=false\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n-Djava.rmi.server.hostname=localhost\n</code></pre> <p>Security note: The default JMX configuration disables authentication and SSL. For production, use port-forwarding or tunnel access instead of exposing JMX externally.</p> <p>Accessing JMX metrics from a local client:</p> <pre><code>kubectl port-forward &lt;pod-name&gt; 5555:5555\njconsole localhost:5555\n</code></pre>"},{"location":"user/monitoring/#prometheus-jmx-exporter","title":"Prometheus JMX Exporter","text":"<p>For production monitoring, the Prometheus JMX Exporter runs as a sidecar container that scrapes JMX metrics from the application and exposes them in Prometheus format.</p> <p>Enable the exporter in <code>values.yaml</code>:</p> <pre><code>prometheus:\n  jmx:\n    enabled: true\n    image: bitnami/jmx-exporter\n    imageTag: 1.1.0\n    imagePullPolicy: Always\n    port: 5556\n    metricRules:\n      - pattern: \".*\"\n    resources:\n      requests:\n        cpu: 10m\n        memory: 100Mi\n      limits:\n        cpu: 100m\n        memory: 100Mi\n</code></pre> <p>Key parameters:</p> <ul> <li><code>prometheus.jmx.enabled</code>: Deploy JMX exporter sidecar (default: <code>false</code>).</li> <li><code>prometheus.jmx.image</code>: Container image for the exporter (default: <code>bitnami/jmx-exporter</code>).</li> <li><code>prometheus.jmx.imageTag</code>: Exporter image tag (default: <code>1.1.0</code>).</li> <li><code>prometheus.jmx.port</code>: HTTP port for metrics endpoint (default: <code>5556</code>).</li> <li><code>prometheus.jmx.metricRules</code>: JMX metric selection and mapping rules.</li> <li><code>prometheus.jmx.resources</code>: Resource requests/limits for the exporter container.</li> </ul>"},{"location":"user/monitoring/#metric-rules","title":"Metric Rules","text":"<p>The <code>metricRules</code> section configures which JMX beans are exposed and how they are mapped to Prometheus metrics. The default configuration uses <code>pattern: \".*\"</code> to export all metrics, but production setups should restrict this to relevant Kafka Streams/producer/consumer metrics.</p> <p>Example rule set:</p> <pre><code>prometheus:\n  jmx:\n    metricRules:\n      - pattern: \"kafka.streams&lt;type=(.+), client-id=(.+), (.+)=(.+)&gt;&lt;&gt;(.+):\"\n        name: kafka_streams_$1_$5\n        labels:\n          client_id: \"$2\"\n          $3: \"$4\"\n      - pattern: \"kafka.producer&lt;type=(.+), client-id=(.+)&gt;&lt;&gt;(.+):\"\n        name: kafka_producer_$1_$3\n        labels:\n          client_id: \"$2\"\n</code></pre> <p>A ConfigMap containing the JMX exporter configuration is generated automatically by the chart and mounted into the sidecar container.</p>"},{"location":"user/monitoring/#prometheus-integration","title":"Prometheus Integration","text":""},{"location":"user/monitoring/#pod-annotations","title":"Pod annotations","text":"<p>For Prometheus instances that use pod annotations for discovery:</p> <pre><code>podAnnotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/path: \"/metrics\"\n  prometheus.io/port: \"5556\"\n</code></pre> <p>This enables scraping the JMX exporter endpoint exposed on <code>prometheus.jmx.port</code>.</p>"},{"location":"user/monitoring/#service-and-podmonitor","title":"Service and PodMonitor","text":"<p>To expose metrics via a Service and integrate with Prometheus Operator using a <code>PodMonitor</code>:</p> <pre><code>service:\n  enabled: true\n  type: ClusterIP\n  labels:\n    monitoring: \"true\"\n\nports:\n  - containerPort: 5556\n    name: metrics\n    protocol: TCP\n    servicePort: 5556\n</code></pre> <p>Example <code>PodMonitor</code>:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: streams-app\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: streams-app\n  podMetricsEndpoints:\n    - port: metrics\n      interval: 30s\n</code></pre>"},{"location":"user/monitoring/#health-checks","title":"Health Checks","text":"<p>Kubernetes uses liveness and readiness probes to determine when a pod is healthy and when it is ready to receive traffic.</p> <p>Liveness probes restart containers that become unhealthy:</p> <pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n</code></pre> <p>Readiness probes gate traffic until the application is ready:</p> <pre><code>readinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 10\n  periodSeconds: 5\n  timeoutSeconds: 3\n  successThreshold: 1\n  failureThreshold: 3\n</code></pre> <p>All standard Kubernetes probe types are supported (<code>httpGet</code>, <code>tcpSocket</code>, <code>exec</code>, and <code>grpc</code> on recent Kubernetes versions). Probes are configured under the corresponding <code>livenessProbe</code> and <code>readinessProbe</code> sections in values.</p>"},{"location":"user/monitoring/#service-and-port-configuration","title":"Service and Port Configuration","text":"<p>Ports and Services control how HTTP APIs and metrics endpoints are exposed:</p> <pre><code>service:\n  enabled: true\n  type: ClusterIP\n\nports:\n  - containerPort: 8080\n    name: http\n    protocol: TCP\n    servicePort: 80\n  - containerPort: 5556\n    name: metrics\n    protocol: TCP\n    servicePort: 5556\n</code></pre> <p>Port mapping reference:</p> <ul> <li><code>jmx.port</code> \u2192 JMX remote port (default <code>5555</code>).</li> <li><code>prometheus.jmx.port</code> \u2192 JMX exporter metrics port (default <code>5556</code>).</li> <li>Additional entries in <code>ports[]</code> \u2192 application-specific ports (e.g. HTTP APIs, custom metrics endpoints).</li> </ul>"},{"location":"user/monitoring/#monitoring-configuration-examples","title":"Monitoring Configuration Examples","text":"<p>Full monitoring stack (JMX exporter, probes, Service, annotations):</p> <pre><code>prometheus:\n  jmx:\n    enabled: true\n    port: 5556\n    metricRules:\n      - pattern: \"kafka.streams&lt;type=stream-metrics, client-id=(.+)&gt;&lt;&gt;(.+):\"\n        name: kafka_streams_$2\n        labels:\n          client_id: \"$1\"\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nlivenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 60\n  periodSeconds: 30\n\nservice:\n  enabled: true\n  type: ClusterIP\n\nports:\n  - containerPort: 8080\n    name: http\n    servicePort: 80\n  - containerPort: 5556\n    name: metrics\n    servicePort: 5556\n\npodAnnotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/path: \"/metrics\"\n  prometheus.io/port: \"5556\"\n</code></pre> <p>Development/debug configuration with JMX only:</p> <pre><code>jmx:\n  enabled: true\n  port: 5555\n  host: localhost\n\nprometheus:\n  jmx:\n    enabled: false\n\nlivenessProbe:\n  tcpSocket:\n    port: 5555\n  initialDelaySeconds: 30\n  periodSeconds: 30\n</code></pre> <p>Minimal production configuration with annotations and resource limits:</p> <pre><code>prometheus:\n  jmx:\n    enabled: true\n    resources:\n      requests:\n        cpu: 10m\n        memory: 100Mi\n      limits:\n        cpu: 100m\n        memory: 100Mi\n\npodAnnotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/port: \"5556\"\n</code></pre>"},{"location":"user/monitoring/#application-specific-considerations","title":"Application-Specific Considerations","text":"<p>Kafka Streams applications expose metrics under several JMX domains, including <code>kafka.streams</code>, <code>kafka.producer</code>, and <code>kafka.consumer</code>. Commonly monitored metrics include:</p> <ul> <li><code>kafka.streams.state</code>: Overall application state (running, rebalancing, error).</li> <li><code>kafka.streams.commit-latency-avg</code>: Average commit latency.</li> <li><code>kafka.consumer.records-lag-max</code>: Maximum records lag per partition.</li> <li><code>kafka.producer.record-send-rate</code>: Producer throughput.</li> </ul> <p>Producer and consumer applications (via <code>producer-app</code> and <code>consumer-app</code> charts) use the same <code>prometheus.jmx</code> structure but may differ in availability patterns (for example, Jobs vs Deployments).</p>"},{"location":"user/monitoring/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues:</p> <ul> <li>No metrics endpoint:<ul> <li>Ensure <code>prometheus.jmx.enabled: true</code>.</li> </ul> </li> <li>Connection refused on JMX port:<ul> <li>Ensure <code>jmx.enabled: true</code> and the port is exposed.</li> </ul> </li> <li>Empty metrics response:<ul> <li>Review <code>metricRules</code> patterns; overly restrictive rules may filter out all metrics.</li> </ul> </li> <li>High exporter CPU usage:<ul> <li>Avoid <code>pattern: \".*\"</code> in production; use targeted patterns instead.</li> </ul> </li> <li>Pod not ready:<ul> <li>Validate liveness/readiness probe configuration and the corresponding application endpoints.</li> </ul> </li> </ul> <p>Verifying metrics export:</p> <pre><code>kubectl port-forward &lt;pod-name&gt; 5556:5556\ncurl http://localhost:5556/metrics\n</code></pre> <p>Debugging JMX connection:</p> <pre><code>kubectl port-forward &lt;pod-name&gt; 5555:5555\njconsole localhost:5555\n</code></pre> <p>If connection fails, verify that JMX is enabled, the port is mapped in <code>ports</code>, and the JVM has been started with the correct JMX system properties.</p>"},{"location":"user/testing/","title":"Testing","text":"<p>The <code>streams-bootstrap</code> Testing Framework provides a comprehensive set of tools for testing Kafka Streams and Producer applications. This framework simplifies both unit and integration testing by providing test abstractions that handle Kafka infrastructure setup, Schema Registry integration, and consumer group verification.</p> <p>The framework supports testing with real Kafka clusters using TestContainers, mock Schema Registry for schema-aware testing, and utilities for verifying application behavior and consumer group states.</p>"},{"location":"user/testing/#core-testing-components","title":"Core Testing Components","text":""},{"location":"user/testing/#kafkatest-base-class","title":"KafkaTest Base Class","text":"<p><code>KafkaTest</code> is an abstract base class that sets up a Kafka environment using TestContainers. It provides:</p> <ul> <li>Kafka container setup</li> <li>Access to bootstrap servers and Schema Registry</li> <li>Methods for waiting on consumer group states</li> <li>Integration with <code>TestSchemaRegistry</code></li> <li>Creation of <code>KafkaTestClient</code> instances</li> </ul>"},{"location":"user/testing/#kafkatestclient","title":"KafkaTestClient","text":"<p><code>KafkaTestClient</code> is a fluent test client that simplifies:</p> <ul> <li>Producing data</li> <li>Consuming records</li> <li>Admin operations</li> <li>Topic creation and verification</li> </ul>"},{"location":"user/testing/#consumergroupverifier","title":"ConsumerGroupVerifier","text":"<p>Provides tools to:</p> <ul> <li>Check if a group is active or closed</li> <li>Get current group state</li> <li>Verify processing completion (lag = 0)</li> <li>Compute lag manually</li> </ul>"},{"location":"user/testing/#unit-testing-with-fluent-kafka-streams-tests","title":"Unit Testing with <code>fluent-kafka-streams-tests</code>","text":"<p>The framework integrates with <code>fluent-kafka-streams-tests</code> for unit testing Kafka Streams topologies.</p>"},{"location":"user/testing/#testschemaregistry","title":"TestSchemaRegistry","text":"<p><code>TestSchemaRegistry</code> provides built-in support for Schema Registry in tests using a mock implementation. It creates isolated Schema Registry instances for testing schema-aware applications.</p>"},{"location":"user/testing/#features","title":"Features:","text":"<ul> <li>Random scoped mock URLs to avoid collisions</li> <li>Support for custom mock URLs</li> <li>Configurable schema providers</li> <li>Compatible with Confluent\u2019s <code>MockSchemaRegistry</code></li> </ul>"},{"location":"user/testing/#example","title":"Example:","text":"<pre><code>// Random scope\nTestSchemaRegistry registry = new TestSchemaRegistry();\n\n// Custom scope\nTestSchemaRegistry registry = new TestSchemaRegistry(\"mock://custom-scope\");\n\n// Default providers\nSchemaRegistryClient client = this.registry.getSchemaRegistryClient();\n\n// With custom providers\nList&lt;SchemaProvider&gt; providers = List.of(new ProtobufSchemaProvider());\nSchemaRegistryClient client = this.registry.getSchemaRegistryClient(this.providers);\n</code></pre>"},{"location":"user/testing/#integration-testing-with-testcontainers","title":"Integration Testing with TestContainers","text":"<p>For integration tests that require a real Kafka environment, the framework provides integration with TestContainers.</p>"},{"location":"user/testing/#single-node-kafka-testing","title":"Single Node Kafka Testing","text":"<p><code>KafkaTest</code> provides a base class for integration tests with a single Kafka broker.</p>"},{"location":"user/testing/#multi-node-cluster-testing","title":"Multi-Node Cluster Testing","text":"<p>For testing with multi-node Kafka clusters, the framework provides <code>ApacheKafkaContainerCluster</code>:</p> <p>Example usage:</p> <pre><code>ApacheKafkaContainerCluster cluster = new ApacheKafkaContainerCluster(\"3.4.0\", 3, 2);\ncluster.\n\nstart();\n\nString bootstrapServers = this.cluster.getBootstrapServers();\n// Run tests...\ncluster.\n\nstop();\n</code></pre>"},{"location":"user/testing/#features_1","title":"Features:","text":"<ul> <li>Configurable broker count</li> <li>Configurable replication factor for internal topics</li> <li>Uses KRaft (no ZooKeeper)</li> <li>Waits for all brokers to be ready before returning</li> </ul>"},{"location":"user/testing/#utilities-for-kafka-testing","title":"Utilities for Kafka Testing","text":""},{"location":"user/testing/#kafkatestclient-operations","title":"KafkaTestClient Operations","text":"<p><code>KafkaTestClient</code> provides a fluent API for common Kafka operations in tests:</p>"},{"location":"user/testing/#topic-management","title":"Topic Management","text":"<pre><code>KafkaTestClient client = newTestClient();\n\n// Create topic with default settings (1 partition, 1 replica)\nclient.createTopic(\"my-topic\");\n\n// Create topic with custom settings\nclient.createTopic(\"my-topic\",\n        KafkaTestClient.defaultTopicSettings()\n        .partitions(3)\n        .replicationFactor(1)\n        .build());\n\n// Create topic with config\nMap&lt;String, String&gt; config = Map.of(\"cleanup.policy\", \"compact\");\nclient.createTopic(\"my-topic\",settings, config);\n\n// Check if topic exists\nboolean exists = this.client.existsTopic(\"my-topic\");\n</code></pre>"},{"location":"user/testing/#data-production","title":"Data Production","text":"<pre><code>client.send()\n        .withKeySerializer(new StringSerializer())\n        .withValueSerializer(new StringSerializer())\n\nto(\"topic-name\",List.of(\n        new SimpleProducerRecord&lt;&gt;(\"key1\",\"value1\"),\n        new SimpleProducerRecord&lt;&gt;(\"key2\",\"value2\")\n        ));\n</code></pre>"},{"location":"user/testing/#data-consumption","title":"Data Consumption","text":"<pre><code>List&lt;ConsumerRecord&lt;String, String&gt;&gt; records = client.read()\n        .withKeyDeserializer(new StringDeserializer())\n        .withValueDeserializer(new StringDeserializer())\n        .from(\"topic-name\", Duration.ofSeconds(10));\n</code></pre>"},{"location":"user/testing/#administrative-operations","title":"Administrative Operations","text":"<p><code>KafkaTestClient</code> provides access to administrative operations through <code>AdminClientX</code>:</p> <pre><code>try(AdminClientX admin = client.admin()){\nTopicClient topicClient = this.admin.getTopicClient();\nConsumerGroupClient consumerGroupClient = this.admin.getConsumerGroupClient();\n}\n</code></pre>"},{"location":"user/testing/#consumer-group-verification","title":"Consumer Group Verification","text":"<p>The framework provides utilities for verifying consumer group states:</p> <pre><code>// Wait for application to become active\nawaitActive(app);\n\n// Wait for completion of processing\nawaitProcessing(app);\n\n// Wait for app to shut down\nawaitClosed(app);\n</code></pre> <p>These methods ensure test reliability by validating consumer group behavior via <code>ConsumerGroupVerifier</code>.</p>"},{"location":"user/concepts/common/","title":"Common concepts","text":""},{"location":"user/concepts/common/#application-types","title":"Application types","text":"<p>The streams-bootstrap framework uses a three-layer application type hierarchy: App \u2192 ConfiguredApp \u2192 ExecutableApp</p>"},{"location":"user/concepts/common/#app","title":"App","text":"<p>The App represents your application logic implementation. Each application type has its own <code>App</code> interface:</p> <ul> <li>StreamsApp \u2013 for Kafka Streams applications</li> <li>ProducerApp \u2013 for producer applications</li> <li>ConsumerApp \u2013 for consumer applications</li> <li>ConsumerProducerApp \u2013 for consumer\u2013producer applications</li> </ul> <p>You implement the appropriate interface to define your application's behavior.</p>"},{"location":"user/concepts/common/#configuredapp","title":"ConfiguredApp","text":"<p>A ConfiguredApp pairs an <code>App</code> with its configuration. Examples include:</p> <ul> <li><code>ConfiguredConsumerApp&lt;T extends ConsumerApp&gt;</code></li> <li><code>ConfiguredProducerApp&lt;T extends ProducerApp&gt;</code></li> </ul> <p>This layer handles Kafka property creation, combining:</p> <ul> <li>base configuration</li> <li>app-specific configuration</li> <li>environment variables</li> <li>runtime configuration</li> </ul>"},{"location":"user/concepts/common/#executableapp","title":"ExecutableApp","text":"<p>An ExecutableApp is a <code>ConfiguredApp</code> with runtime configuration applied, making it ready to execute. It can create:</p> <ul> <li>a Runner for running the application</li> <li>a CleanUpRunner for cleanup operations</li> </ul> <p>The <code>KafkaApplication</code> base class orchestrates the creation of these components through methods such as:</p> <ul> <li><code>createConfiguredApp()</code></li> <li><code>createExecutableApp()</code></li> </ul>"},{"location":"user/concepts/common/#usage-pattern","title":"Usage Pattern","text":"<ol> <li>You implement an App.</li> <li>The framework wraps it in a ConfiguredApp, applying the configuration.</li> <li> <p>Runtime configuration is then applied to create an ExecutableApp, which can be:</p> </li> <li> <p>run, or</p> </li> <li>cleaned up.</li> </ol>"},{"location":"user/concepts/common/#application-lifecycle","title":"Application lifecycle","text":"<p>Applications built with streams-bootstrap follow a defined lifecycle with specific states and transitions.</p> <p>The framework manages this lifecycle through the KafkaApplication base class and provides several extension points for customization.</p> Phase Description Entry Point Initialization Parse CLI arguments, inject environment variables, configure application <code>startApplication()</code> or <code>startApplicationWithoutExit()</code> Preparation Execute pre-run/pre-clean hooks <code>onApplicationStart()</code>, <code>prepareRun()</code>, <code>prepareClean()</code> Execution Run main application logic or cleanup operations <code>run()</code>, <code>clean()</code>, <code>reset()</code> Shutdown Stop runners, close resources, cleanup <code>stop()</code>, <code>close()</code>"},{"location":"user/concepts/common/#running-an-application","title":"Running an application","text":"<p>Applications built with streams-bootstrap can be started in two primary ways:</p> <ul> <li>Via Command Line Interface: When packaged as a runnable JAR (for example, in a container),   the <code>run</code> command is the default entrypoint. An example invocation:</li> </ul> <p><code>bash   java -jar example-app.jar \\       run \\       --bootstrap-servers kafka:9092 \\       --input-topics input-topic \\       --output-topic output-topic \\       --schema-registry-url http://schema-registry:8081</code></p> <ul> <li>Programmatically: The application subclass calls <code>startApplication(args)</code> on startup. Example for a Kafka Streams   application:</li> </ul> <pre><code>  public static void main(final String[] args) {\n    new MyStreamsApplication().startApplication(args);\n}\n  ```\n\n### Cleaning an application\n\nThe framework provides a built-in mechanism to clean up all resources associated with an application.\n\nWhen the cleanup operation is triggered, the following resources are removed:\n\n| Resource Type       | Description                                               | Streams Apps | Producer Apps |\n|---------------------|-----------------------------------------------------------|--------------|---------------|\n| Output Topics       | The main output topic of the application                  | \u2713            | \u2713             |\n| Intermediate Topics | Topics for stream operations like `through()`             | \u2713            | N/A           |\n| Internal Topics     | Topics for state stores or repartitioning (Kafka Streams) | \u2713            | N/A           |\n| Consumer Groups     | Consumer group metadata                                   | \u2713            | N/A           |\n| Schema Registry     | All registered schemas                                    | \u2713            | \u2713             |\n\nCleanup can be triggered:\n\n- **Via Command Line**: Helm cleanup jobs\n- **Programmatically**:\n\n```java\n// For streams applications\ntry(StreamsCleanUpRunner cleanUpRunner = streamsApp.createCleanUpRunner()){\n        cleanUpRunner.\n\nclean();\n}\n\n// For producer applications\n        try(\nCleanUpRunner cleanUpRunner = producerApp.createCleanUpRunner()){\n        cleanUpRunner.\n\nclean();\n}\n</code></pre> <p>The framework ensures that cleanup operations are idempotent, meaning they can be safely retried without causing additional issues.</p>"},{"location":"user/concepts/common/#configuration","title":"Configuration","text":"<p>Kafka properties are applied in the following order (later values override earlier ones):</p> <ol> <li>Base configuration</li> <li>App config from .createKafkaProperties()</li> <li>Environment variables (<code>KAFKA_</code>)</li> <li>Runtime args (--bootstrap-servers, etc.)</li> <li>Serialization config from ProducerApp.defaultSerializationConfig() or StreamsApp.defaultSerializationConfig()</li> <li>CLI overrides via --kafka-config</li> </ol> <p>The framework automatically parses environment variables with the <code>APP_ prefix</code> (configurable via <code>ENV_PREFIX</code>). Environment variables are converted to CLI arguments:</p> <pre><code>APP_BOOTSTRAP_SERVERS       \u2192      --bootstrap-servers\nAPP_SCHEMA_REGISTRY_URL     \u2192      --schema-registry-url\nAPP_OUTPUT_TOPIC            \u2192      --output-topic\n</code></pre> <p>Additionally, Kafka-specific environment variables with the <code>KAFKA_</code> prefix are automatically added to the Kafka configuration.</p>"},{"location":"user/concepts/common/#command-line-interface","title":"Command line interface","text":"<p>The framework provides a unified command-line interface for application configuration.</p>"},{"location":"user/concepts/common/#cli-commands","title":"CLI Commands","text":"<ul> <li><code>run</code>: Run the application</li> <li><code>clean</code>: Delete topics and consumer groups</li> <li><code>reset</code>: Reset internal state and offsets (for Streams apps)</li> </ul>"},{"location":"user/concepts/common/#common-cli-configuration-options","title":"Common CLI Configuration Options","text":"<ul> <li><code>--bootstrap-servers</code>: Kafka bootstrap servers (required)</li> <li><code>--schema-registry-url</code>: URL for Avro serialization</li> <li><code>--kafka-config</code>: Key-value Kafka configuration</li> <li><code>--output-topic</code>: Main output topic</li> <li><code>--labeled-output-topics</code>: Named output topics</li> <li><code>--input-topics</code>: Input topics (for Streams apps)</li> <li><code>--input-pattern</code>: Input topic pattern (for Streams apps)</li> <li><code>--application-id</code>: Unique app ID (for Streams apps)</li> </ul>"},{"location":"user/concepts/producer/","title":"Producer applications","text":"<p>Producer applications are applications that generate data and send it to Kafka topics. They can be used to produce messages from various sources, such as databases, files, or real-time events.</p> <p>The <code>streams-bootstrap</code> framework provides a structured way to build Kafka producer applications with built-in configuration handling, command-line support, and resource lifecycle management.</p>"},{"location":"user/concepts/producer/#application-lifecycle","title":"Application lifecycle","text":""},{"location":"user/concepts/producer/#running-an-application","title":"Running an application","text":"<p>Producer applications are executed using the <code>ProducerRunner</code>, which runs the producer logic defined by the application.</p> <p>Unlike Kafka Streams applications, producer applications typically:</p> <ul> <li>Run to completion and terminate automatically, or</li> <li>Run continuously when implemented as long-lived services</li> </ul> <p>The execution model is fully controlled by the producer implementation and its runnable logic.</p>"},{"location":"user/concepts/producer/#cleaning-an-application","title":"Cleaning an application","text":"<p>Producer applications support a dedicated <code>clean</code> command that removes Kafka-related resources created by the application.</p> <pre><code>java -jar my-producer-app.jar \\\n  --bootstrap-servers localhost:9092 \\\n  --output-topic my-topic \\\n  clean\n</code></pre> <p>The clean process can perform the following operations:</p> <ul> <li>Delete output topics</li> <li>Delete registered schemas from Schema Registry</li> <li>Execute custom cleanup hooks defined by the application</li> </ul> <p>Applications can register custom cleanup logic by overriding <code>setupCleanUp</code>:</p> <pre><code>\n@Override\npublic void setupCleanUp(final EffectiveAppConfiguration configuration) {\n    configuration.addCleanupHook(() -&gt; {\n        // Custom cleanup logic\n    });\n}\n</code></pre>"},{"location":"user/concepts/producer/#configuration","title":"Configuration","text":""},{"location":"user/concepts/producer/#serialization-configuration","title":"Serialization configuration","text":"<p>Producer applications define key and value serialization using the <code>defaultSerializationConfig()</code> method in their <code>ProducerApp</code> implementation.</p> <pre><code>\n@Override\npublic SerializerConfig defaultSerializationConfig() {\n    return new SerializerConfig(StringSerializer.class, SpecificAvroSerializer.class);\n}\n</code></pre> <p>Common serializer combinations include:</p> Key Serializer Value Serializer Use case <code>StringSerializer</code> <code>StringSerializer</code> Simple string messages <code>StringSerializer</code> <code>SpecificAvroSerializer</code> Avro with schema evolution <code>StringSerializer</code> <code>GenericAvroSerializer</code> Dynamic Avro schemas <code>ByteArraySerializer</code> <code>ByteArraySerializer</code> Binary data"},{"location":"user/concepts/producer/#custom-kafka-properties","title":"Custom Kafka properties","text":"<p>Producer-specific Kafka configuration can be customized by overriding <code>createKafkaProperties()</code>:</p> <pre><code>\n@Override\npublic Map&lt;String, Object&gt; createKafkaProperties() {\n    return Map.of(\n            ProducerConfig.ACKS_CONFIG, \"all\",\n            ProducerConfig.RETRIES_CONFIG, 3,\n            ProducerConfig.BATCH_SIZE_CONFIG, 16384,\n            ProducerConfig.LINGER_MS_CONFIG, 5\n    );\n}\n</code></pre> <p>These properties are merged with the framework defaults and CLI-provided configuration.</p>"},{"location":"user/concepts/producer/#schema-registry-integration","title":"Schema Registry integration","text":"<p>When the <code>--schema-registry-url</code> option is provided:</p> <ul> <li>Schemas are registered automatically during application startup</li> <li>Schema cleanup is handled as part of the <code>clean</code> command</li> <li>Schema evolution is fully supported</li> </ul>"},{"location":"user/concepts/producer/#command-line-interface","title":"Command line interface","text":"<p>Producer applications inherit standard CLI options from <code>KafkaApplication</code>.</p> <pre><code>--bootstrap-servers         Kafka bootstrap servers (comma-separated)          (Required)\n--bootstrap-server          Alias for --bootstrap-servers                      (Required)\n--schema-registry-url       URL of the Schema Registry                         (Optional)\n--kafka-config              Additional Kafka config (key=value,...)            (Optional)\n--output-topic              Default output topic                               (Optional)\n--labeled-output-topics     Named output topics (label1=topic1,...)            (Optional)\n</code></pre>"},{"location":"user/concepts/producer/#deployment","title":"Deployment","text":"<p>TODO</p>"},{"location":"user/concepts/streams/","title":"Streams applications","text":"<p>Streams apps are applications that process data in real-time as it flows through Kafka topics. They can be used to filter, transform, aggregate, or enrich data streams. Streams apps can also produce new messages to other topics based on the processed data.</p>"},{"location":"user/concepts/streams/#application-lifecycle","title":"Application lifecycle","text":""},{"location":"user/concepts/streams/#running-an-application","title":"Running an application","text":"<p>Kafka Streams applications are started via the <code>KafkaStreamsApplication</code> entry point:</p> <pre><code>public static void main(final String[] args) {\n    new MyStreamsApplication().startApplication(args);\n}\n</code></pre> <p>When an application is started, the framework performs the following steps:</p> <ul> <li>Parse command-line arguments and environment variables</li> <li>Create a <code>StreamsApp</code> instance</li> <li>Wrap it in a <code>ConfiguredStreamsApp</code></li> <li>Convert it into an <code>ExecutableStreamsApp</code></li> <li>Start execution using the <code>StreamsRunner</code></li> </ul>"},{"location":"user/concepts/streams/#resetting-an-application","title":"Resetting an application","text":"<p>Streams applications support a dedicated <code>reset</code> operation that clears processing state while preserving the application definition and configuration. This is useful for reprocessing input data from the beginning.</p> <p>When a reset is triggered, the following resources are affected:</p> Resource Action State stores Cleared locally, changelog topics deleted Internal topics Deleted (e.g. repartition topics) Consumer offsets Reset to earliest for input topics Output topics Preserved Application config Preserved <p>Triggering a reset via CLI:</p> <pre><code>java -jar my-streams-app.jar reset\n</code></pre> <p>Triggering a reset programmatically:</p> <pre><code>try(StreamsCleanUpRunner cleanUpRunner = streamsApp.createCleanUpRunner()){\n        cleanUpRunner.\n\nreset();\n}\n</code></pre> <p>After a reset, the application can be started again and will reprocess all input data.</p>"},{"location":"user/concepts/streams/#cleaning-an-application","title":"Cleaning an application","text":"<p>The <code>clean</code> command removes Kafka-related resources created by the application:</p> <pre><code>java -jar my-streams-app.jar clean\n</code></pre> <p>This includes:</p> <ul> <li>Consumer groups</li> <li>Internal topics</li> <li>Output topics (unless explicitly preserved)</li> </ul>"},{"location":"user/concepts/streams/#configuration","title":"Configuration","text":""},{"location":"user/concepts/streams/#topics","title":"Topics","text":"<p>Streams applications support flexible topic configuration:</p> <ul> <li><code>--input-topics</code>: Comma-separated list of input topics</li> <li><code>--input-pattern</code>: Regex pattern for input topics</li> <li><code>--output-topic</code>: Default output topic</li> <li><code>--error-topic</code>: Topic for error records</li> <li><code>--labeled-input-topics</code>: Named input topics with different message types</li> <li><code>--labeled-input-patterns</code>: Additional labeled input topic patterns</li> <li><code>--labeled-output-topics</code>: Named output topics with different message types</li> </ul>"},{"location":"user/concepts/streams/#application-id","title":"Application ID","text":"<ul> <li><code>--application-id</code>: Unique Kafka Streams application ID</li> </ul> <p>If not provided, the framework generates a deterministic application ID.</p>"},{"location":"user/concepts/streams/#kafka-properties","title":"Kafka properties","text":"<p>Additional Kafka Streams configuration can be supplied using:</p> <ul> <li><code>--kafka-config &lt;key=value,...&gt;</code></li> </ul> <p>The framework applies the following defaults:</p> <pre><code>processing.guarantee=exactly_once_v2\nproducer.max.in.flight.requests.per.connection=1\nproducer.acks=all\nproducer.compression.type=gzip\n</code></pre>"},{"location":"user/concepts/streams/#lifecycle-hooks","title":"Lifecycle hooks","text":""},{"location":"user/concepts/streams/#setup","title":"Setup","text":"<p>Custom uncaught exception handling can be provided by overriding the default handler:</p> <pre><code>\n@Override\nprivate StreamsUncaughtExceptionHandler createUncaughtExceptionHandler() {\n    return new MyCustomExceptionHandler();\n}\n</code></pre>"},{"location":"user/concepts/streams/#clean-up","title":"Clean up","text":"<p>Applications can observe Kafka Streams state transitions by registering a state listener:</p> <pre><code>\n@Override\nprivate StateListener createStateListener() {\n    return new MyCustomStateListener();\n}\n</code></pre>"},{"location":"user/concepts/streams/#execution-options","title":"Execution options","text":""},{"location":"user/concepts/streams/#on-start","title":"On start","text":"<p>Custom logic can be executed once Kafka Streams has fully started:</p> <pre><code>\n@Override\nprivate void onStreamsStart(final RunningStreams runningStreams) {\n    // Custom startup logic\n}\n</code></pre>"},{"location":"user/concepts/streams/#application-server","title":"Application server","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#state-listener","title":"State listener","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#uncaught-exception-handler","title":"Uncaught exception handler","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#closing-options","title":"Closing options","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#command-line-interface","title":"Command line interface","text":"<p>Streams applications inherit standard CLI options from <code>KafkaStreamsApplication</code>.</p> Option Description Default <code>--bootstrap-servers</code> Kafka bootstrap servers (comma-separated) Required <code>--schema-registry-url</code> URL of the Schema Registry None <code>--application-id</code> Kafka Streams application ID Auto-generated <code>--volatile-group-instance-id</code> Use volatile group instance ID false"},{"location":"user/concepts/streams/#deployment","title":"Deployment","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#kafka-streams-extensions","title":"Kafka Streams extensions","text":"<p>The framework provides several extensions that simplify working with Kafka Streams.</p>"},{"location":"user/concepts/streams/#simple-topic-access","title":"Simple topic access","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#error-handling","title":"Error handling","text":"<p>TODO</p>"},{"location":"user/concepts/streams/#serde-auto-configuration","title":"Serde auto configuration","text":"<p>TODO</p>"},{"location":"user/deployment/kubernetes/","title":"Deployment to Kubernetes","text":"<p>The <code>streams-bootstrap</code> framework provides support for deploying applications to Kubernetes using Helm charts. The charts cover Kafka Streams, producer, consumer, and producer-consumer applications and offer standardized solutions for autoscaling, monitoring, and state persistence.</p>"},{"location":"user/deployment/kubernetes/#core-capabilities","title":"Core capabilities","text":"<ul> <li>Autoscaling \u2013 KEDA-based horizontal scaling driven by Kafka consumer lag</li> <li>Monitoring \u2013 JMX metrics export with Prometheus integration</li> <li>Persistence \u2013 Persistent volumes for Kafka Streams state stores</li> </ul>"},{"location":"user/deployment/kubernetes/#helm-charts","title":"Helm charts","text":"<p>The framework ships a set of Helm charts tailored to different application types:</p> Chart name Purpose Kubernetes workload types <code>streams-app</code> Deploy Kafka Streams applications <code>Deployment</code>, <code>StatefulSet</code> <code>producer-app</code> Deploy Kafka Producer applications <code>Deployment</code>, <code>Job</code>, <code>CronJob</code> <code>consumer-app</code> Deploy Kafka Consumer applications <code>Deployment</code> <code>consumerproducer-app</code> Deploy batch / consumer\u2013producer applications <code>Deployment</code> <code>*-cleanup-job</code> Clean Kafka resources before deployment <code>Job</code> (Helm hooks) <p>Cleanup charts (for example <code>streams-app-cleanup-job</code> and <code>producer-app-cleanup-job</code>) remove Kafka topics, consumer groups, and Schema Registry subjects associated with a release before a new deployment.</p>"},{"location":"user/deployment/kubernetes/#chart-repository-and-installation","title":"Chart repository and installation","text":"<p>The Helm charts are published as a Helm repository:</p> <pre><code>helm repo add streams-bootstrap https://bakdata.github.io/streams-bootstrap/\nhelm repo update\n</code></pre> <p>A Streams application can then be installed with:</p> <pre><code>helm install my-app bakdata-common/streams-app --values my-values.yaml\n</code></pre>"},{"location":"user/deployment/kubernetes/#deployment-patterns","title":"Deployment patterns","text":""},{"location":"user/deployment/kubernetes/#streams-applications-streams-app","title":"Streams applications (<code>streams-app</code>)","text":"<p>Streams applications support both stateless and stateful deployment modes:</p> <ul> <li> <p>Deployment</p> <ul> <li>Used for stateless applications or when state is stored externally</li> <li>Enabled when <code>statefulSet: false</code> or <code>persistence.enabled: false</code></li> </ul> </li> <li> <p>StatefulSet</p> <ul> <li>Used for stateful Kafka Streams applications with local state stores</li> <li>Enabled when <code>statefulSet: true</code> and <code>persistence.enabled: true</code></li> <li>Each pod receives a dedicated <code>PersistentVolumeClaim</code> for RocksDB state</li> </ul> </li> </ul> <p>This allows choosing between elasticity (Deployment) and stronger data locality guarantees (StatefulSet).</p>"},{"location":"user/deployment/kubernetes/#producer-applications-producer-app","title":"Producer applications (<code>producer-app</code>)","text":"<p>Producer applications support multiple execution models:</p> Mode Use case Resource type Deployment Long-running or continuous producer <code>apps/v1/Deployment</code> Job One-time run or backfill <code>batch/v1/Job</code> CronJob Scheduled periodic execution <code>batch/v1/CronJob</code>"},{"location":"user/deployment/kubernetes/#consumer-and-consumerproducer-applications","title":"Consumer and consumer\u2013producer applications","text":"<ul> <li> <p><code>consumer-app</code></p> <ul> <li>Deployed as a <code>Deployment</code></li> <li>Uses Kafka consumer groups for parallel consumption</li> </ul> </li> <li> <p><code>consumerproducer-app</code></p> <ul> <li>Deployed as a <code>Deployment</code></li> <li>Typically used for batch-style read\u2013process\u2013write workloads</li> </ul> </li> </ul>"},{"location":"user/deployment/kubernetes/#cleanup-jobs","title":"Cleanup jobs","text":"<p>Cleanup charts are executed as Helm hook Jobs:</p> <ul> <li>Run as <code>pre-install</code> or <code>pre-upgrade</code> hooks</li> <li>Remove:<ul> <li>Kafka topics</li> <li>Consumer groups</li> <li>Schema Registry subjects</li> </ul> </li> </ul> <p>This ensures a clean starting point for reprocessing or redeployment scenarios.</p>"},{"location":"user/deployment/kubernetes/#configuration-structure","title":"Configuration structure","text":"<p>All charts share a common configuration structure, with chart-specific extensions:</p> Section Purpose Examples <code>image</code>, <code>imageTag</code> Container image configuration <code>streamsApp</code>, <code>latest</code> <code>kafka.*</code> Kafka connection and topic configuration <code>bootstrapServers</code>, <code>inputTopics</code> <code>commandLine.*</code> CLI arguments passed to the application <code>MY_PARAM: \"value\"</code> <code>env.*</code> Additional environment variables <code>MY_ENV: foo</code> <code>secrets.*</code> Inline secret values Tokens, passwords <code>secretRefs.*</code> References to existing <code>Secret</code> objects External credentials <code>resources.*</code> CPU and memory requests/limits <code>requests.cpu: 200m</code> <code>autoscaling.*</code> KEDA autoscaling configuration <code>lagThreshold</code>, <code>minReplicas</code> <code>persistence.*</code> Streams state-store persistence <code>enabled: true</code>, <code>size: 1Gi</code> <code>jmx.*</code>, <code>prometheus.*</code> JMX exporter and Prometheus configuration <code>jmx.enabled: true</code> <code>statefulSet</code> Toggle <code>StatefulSet</code> vs <code>Deployment</code> <code>true</code> / <code>false</code>"},{"location":"user/deployment/kubernetes/#environment-variable-mapping","title":"Environment variable mapping","text":"<p>Helm values are translated into environment variables using a configurable prefix:</p> <pre><code>configurationEnvPrefix: \"APP\"\n\ncommandLine:\n  MY_PARAM: \"value\"\nkafka:\n  inputTopics: [ \"input\" ]\n  outputTopic: \"output\"\n</code></pre> <p>This results in:</p> <ul> <li><code>APP_MY_PARAM=value</code></li> <li><code>APP_INPUT_TOPICS=input</code></li> <li><code>APP_OUTPUT_TOPIC=output</code></li> </ul> <p>Kafka client configuration uses the <code>KAFKA_</code> prefix:</p> <pre><code>kafka:\n  config:\n    max.poll.records: 500\n</code></pre> <p>Becomes:</p> <ul> <li><code>KAFKA_MAX_POLL_RECORDS=500</code></li> </ul>"},{"location":"user/deployment/kubernetes/#autoscaling","title":"Autoscaling","text":"<p>Autoscaling is implemented using Kubernetes Event-Driven Autoscaling (KEDA). When enabled, KEDA monitors Kafka consumer lag and adjusts the number of replicas accordingly.</p> <p>Autoscaling is disabled by default.</p>"},{"location":"user/deployment/kubernetes/#enabling-autoscaling","title":"Enabling autoscaling","text":"<pre><code>autoscaling:\n  enabled: true\n  lagThreshold: \"1000\"\n  minReplicas: 0\n  maxReplicas: 5\n</code></pre> <p>When enabled, the chart creates a KEDA <code>ScaledObject</code> and omits a fixed <code>replicaCount</code> from the workload specification.</p>"},{"location":"user/deployment/kubernetes/#scaling-behavior","title":"Scaling behavior","text":"<p>KEDA computes the desired number of replicas as:</p> <pre><code>desiredReplicas = ceil(totalLag / lagThreshold)\n</code></pre> <p>subject to <code>minReplicas</code> and <code>maxReplicas</code>.</p>"},{"location":"user/deployment/kubernetes/#integration-with-persistence","title":"Integration with persistence","text":"<p>When persistence is enabled for Streams applications, autoscaling targets a <code>StatefulSet</code>. Each replica receives its own <code>PersistentVolumeClaim</code>.</p> <p>Note: Scale-down operations remove pods and their PVCs. Backup and recovery strategies should be considered.</p>"},{"location":"user/deployment/kubernetes/#monitoring","title":"Monitoring","text":"<p>Monitoring is based on JMX metrics and Prometheus scraping:</p> <ul> <li><code>jmx.enabled: true</code> enables Kafka client and Streams metrics</li> <li><code>prometheus.jmx.enabled: true</code> adds a Prometheus JMX exporter sidecar</li> <li>Metrics are exposed on a dedicated <code>/metrics</code> endpoint</li> </ul> <p>Collected metrics include consumer lag, processing rates, and RocksDB statistics.</p>"},{"location":"user/deployment/kubernetes/#persistence","title":"Persistence","text":"<p>Persistence is configured via the <code>persistence.*</code> section (Streams applications only):</p> <pre><code>persistence:\n  enabled: true\n  size: 1Gi\n  storageClassName: standard\n</code></pre> <p>When enabled together with <code>statefulSet: true</code>, each pod receives a dedicated volume for local state storage. This enables:</p> <ul> <li>Faster restarts due to warm state</li> <li>Improved recovery semantics for stateful topologies</li> </ul> <p>If persistence is disabled, applications behave as stateless deployments and rely on Kafka changelogs for state reconstruction.</p>"},{"location":"user/deployment/local/","title":"Local deployment","text":"<p>Applications can be run locally for development and testing purposes. This can be done programmatically within your code.</p>"},{"location":"user/deployment/local/#programmatic-local-execution","title":"Programmatic Local Execution","text":"<p>Here is an example of how to run a producer application programmatically. This is useful for simple applications or for testing.</p> <pre><code>try(final KafkaProducerApplication&lt;?&gt; app = new SimpleKafkaProducerApplication&lt;&gt;(() -&gt;\n        new ProducerApp() {\n            @Override\n            public ProducerRunnable buildRunnable(final ProducerBuilder builder) {\n                return () -&gt; {\n                    try (final Producer&lt;Object, Object&gt; producer = builder.createProducer()) {\n                        // Producer logic\n                    }\n                };\n            }\n\n            @Override\n            public SerializerConfig defaultSerializationConfig() {\n                return new SerializerConfig(StringSerializer.class, StringSerializer.class);\n            }\n        }\n)){\n        app.\n\nsetBootstrapServers(\"localhost:9092\");\n    app.\n\nsetOutputTopic(\"output-topic\");\n    app.\n\nrun();\n}\n</code></pre>"},{"location":"user/deployment/local/#command-line-execution","title":"Command Line Execution","text":"<p>You can also run the application from the command line by packaging it as a JAR file.</p> <pre><code>java -jar my-producer-app.jar --bootstrap-servers localhost:9092 --output-topic my-topic run\n</code></pre>"},{"location":"user/examples/interactive-queries/","title":"Interactive queries","text":""},{"location":"user/examples/word-count/","title":"Word count","text":""},{"location":"user/extensions/large-messages/","title":"Large messages","text":""},{"location":"user/extensions/large-messages/#overview","title":"Overview","text":"<p>The Large Messages Extension adds support for handling messages that exceed Kafka's size limitations by using external storage mechanisms with automatic cleanup. It integrates with the streams-bootstrap framework to transparently manage:</p> <ul> <li>large message serialization</li> <li>large message deserialization</li> <li>external storage cleanup</li> </ul> <p>For both producer and streams applications.</p>"},{"location":"user/extensions/large-messages/#core-components","title":"Core Components","text":""},{"location":"user/extensions/large-messages/#largemessageapputils","title":"LargeMessageAppUtils","text":"<p><code>LargeMessageAppUtils</code> is the central utility for managing large message cleanup operations. It provides factory methods for creating topic hooks that automatically clean up externally stored large message files when topics are deleted.</p> <p>Key methods:</p> <ul> <li><code>createTopicHook(Map&lt;String, Object&gt; kafkaProperties)</code>   Creates a cleanup hook from Kafka properties.</li> <li><code>createTopicHook(AppConfiguration&lt;?&gt; configuration)</code>   Creates a cleanup hook from application configuration.</li> <li><code>registerTopicHook()</code>   Registers the cleanup hook with a cleanup configuration.</li> </ul>"},{"location":"user/extensions/large-messages/#largemessageproducerapp","title":"LargeMessageProducerApp","text":"<p><code>LargeMessageProducerApp</code> extends <code>ProducerApp</code> to automatically handle cleanup of large message files for producer applications.</p> <p>When a topic is deleted, it removes all associated large message files stored externally via the <code>LargeMessageSerializer</code>.</p> <p>The interface automatically registers the cleanup hook inside <code>setupCleanUp()</code>:</p>"},{"location":"user/extensions/large-messages/#largemessagestreamsapp","title":"LargeMessageStreamsApp","text":"<p><code>LargeMessageStreamsApp</code> extends <code>StreamsApp</code> to provide automatic cleanup for Kafka Streams applications using large messages.</p> <p>When streams topics are cleaned up, the extension ensures corresponding external large message files are also removed.</p> <p>It also registers the cleanup hook during <code>setupCleanUp()</code>:</p>"},{"location":"user/extensions/large-messages/#implementation-details","title":"Implementation Details","text":""},{"location":"user/extensions/large-messages/#topic-hook-mechanism","title":"Topic Hook Mechanism","text":"<p>Cleanup is implemented through the <code>LargeMessageTopicHook</code> class, which implements the <code>TopicHook</code> interface.</p> <p>When a topic is deleted, the hook's <code>deleted()</code> method is triggered, which calls <code>deleteAllFiles()</code> on the configured <code>LargeMessageStoringClient</code>.</p>"},{"location":"user/extensions/large-messages/#configuration-requirements","title":"Configuration Requirements","text":"<p>The large message extension requires Kafka properties needed to build an <code>AbstractLargeMessageConfig</code>.</p> <p>This configuration is used to instantiate the appropriate <code>LargeMessageStoringClient</code> for the storage backend.</p>"},{"location":"user/extensions/large-messages/#usage-examples","title":"Usage Examples","text":""},{"location":"user/extensions/large-messages/#producer-application","title":"Producer Application","text":"<pre><code>public class MyLargeMessageProducer extends KafkaProducerApplication&lt;LargeMessageProducerApp&gt; {\n    @Override\n    public LargeMessageProducerApp createApp() {\n        return new LargeMessageProducerApp() {\n\n            @Override\n            public ProducerRunnable buildRunnable(final ProducerBuilder builder) {\n                return () -&gt; {\n                    try (final Producer&lt;Object, Object&gt; producer = builder.createProducer()) {\n                        // Producer logic with large message support  \n                    }\n                };\n            }\n\n            @Override\n            public SerializerConfig defaultSerializationConfig() {\n                // Configure LargeMessageSerializer  \n                return new SerializerConfig(LargeMessageSerializer.class, LargeMessageSerializer.class);\n            }\n        };\n    }\n}\n\n</code></pre>"},{"location":"user/getting-started/quick-start/","title":"Quick Start","text":"<p>This page shows how to add <code>streams-bootstrap</code> to a project and how to create and run a minimal application.</p>"},{"location":"user/getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Java 17</li> <li>Apache Kafka cluster (brokers reachable from the application)</li> <li>Access to Maven Central</li> <li><code>streams-bootstrap-cli</code> dependency (see Setup for Gradle/Maven snippets)</li> </ul>"},{"location":"user/getting-started/quick-start/#minimal-kafka-streams-application","title":"Minimal Kafka Streams Application","text":"<p>Create a subclass of <code>KafkaStreamsApplication</code> and implement the required methods.</p> <pre><code>import com.bakdata.kafka.streams.KafkaStreamsApplication;\nimport com.bakdata.kafka.streams.SerdeConfig;\nimport com.bakdata.kafka.streams.StreamsApp;\nimport com.bakdata.kafka.streams.StreamsTopicConfig;\nimport com.bakdata.kafka.streams.kstream.KStreamX;\nimport com.bakdata.kafka.streams.kstream.StreamsBuilderX;\nimport java.util.Map;\nimport org.apache.kafka.common.serialization.Serdes.StringSerde;\n\npublic class MyStreamsApplication extends KafkaStreamsApplication&lt;StreamsApp&gt; {\n\n    public static void main(final String[] args) {\n        new MyStreamsApplication().startApplication(args);\n    }\n\n    @Override\n    public StreamsApp createApp() {\n        return new StreamsApp() {\n            @Override\n            public void buildTopology(final StreamsBuilderX builder) {\n                final KStreamX&lt;String, String&gt; input = builder.streamInput();\n\n                // topology definition\n\n                input.toOutputTopic();\n            }\n\n            @Override\n            public String getUniqueAppId(final StreamsTopicConfig topics) {\n                return \"streams-bootstrap-app-\" + topics.getOutputTopic();\n            }\n\n            @Override\n            public SerdeConfig defaultSerializationConfig() {\n                return new SerdeConfig(StringSerde.class, StringSerde.class);\n            }\n\n            @Override\n            public Map&lt;String, Object&gt; createKafkaProperties() {\n                return Map.of(\n                        // additional Kafka properties\n                );\n            }\n        };\n    }\n}\n</code></pre>"},{"location":"user/getting-started/quick-start/#running-the-application","title":"Running the Application","text":""},{"location":"user/getting-started/quick-start/#via-command-line-interface","title":"Via Command Line Interface","text":"<p>When packaged as a runnable JAR (for example, in a container), the <code>run</code> command is the default entrypoint:</p> <pre><code>java -jar my-streams-app.jar \\\n    run \\\n    --bootstrap-servers kafka:9092 \\\n    --input-topics input-topic \\\n    --output-topic output-topic \\\n    --schema-registry-url http://schema-registry:8081\n</code></pre> <p>Additional subcommands such as <code>clean</code> and <code>reset</code> are available for lifecycle management.</p>"},{"location":"user/getting-started/quick-start/#from-the-main-method","title":"From the <code>main</code> Method","text":"<p>In the <code>main</code> method, the application subclass starts the framework via:</p> <pre><code>public static void main(final String[] args) {\n    new MyStreamsApplication().startApplication(args);\n}\n</code></pre> <p>This delegates configuration loading, lifecycle handling, and shutdown to <code>streams-bootstrap</code>.</p>"},{"location":"user/getting-started/setup/","title":"Setup","text":"<p>This page describes dependency setup, configuration options, commands, and Helm-based deployment for <code>streams-bootstrap</code>.</p>"},{"location":"user/getting-started/setup/#dependencies","title":"Dependencies","text":""},{"location":"user/getting-started/setup/#gradle","title":"Gradle","text":"<pre><code>implementation group: 'com.bakdata.kafka', name: 'streams-bootstrap-cli', version: '6.1.0'\n</code></pre> <p>With Kotlin DSL:</p> <pre><code>implementation(group = \"com.bakdata.kafka\", name = \"streams-bootstrap-cli\", version = \"6.1.0\")\n</code></pre>"},{"location":"user/getting-started/setup/#maven","title":"Maven","text":"<pre><code>\n&lt;dependency&gt;\n    &lt;groupId&gt;com.bakdata.kafka&lt;/groupId&gt;\n    &lt;artifactId&gt;streams-bootstrap-cli&lt;/artifactId&gt;\n    &lt;version&gt;6.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>For other build tools or versions, refer to the latest version in MvnRepository.</p>"},{"location":"user/getting-started/setup/#kafka-streams-applications","title":"Kafka Streams Applications","text":"<p>Create a subclass of <code>KafkaStreamsApplication</code> and provide a <code>StreamsApp</code> via <code>createApp()</code>.</p> <p>Key responsibilities of <code>StreamsApp</code>:</p> <ul> <li>Define the topology in <code>buildTopology(StreamsBuilderX builder)</code></li> <li>Provide a unique application id in <code>getUniqueAppId(StreamsTopicConfig topics)</code></li> <li>Configure default key/value SerDes via <code>defaultSerializationConfig()</code></li> <li>Optionally provide extra Kafka properties via <code>createKafkaProperties()</code></li> </ul> <p>A complete example is shown in the Quick Start page.</p>"},{"location":"user/getting-started/setup/#configuration-options-streams","title":"Configuration Options (Streams)","text":"<p>The following CLI options are available:</p> <ul> <li><code>--bootstrap-servers</code>, <code>--bootstrap-server</code>: List of Kafka bootstrap servers (comma-separated) (required)</li> <li><code>--schema-registry-url</code>: The URL of the Schema Registry</li> <li><code>--kafka-config</code>: Kafka Streams configuration (<code>&lt;String=String&gt;[,&lt;String=String&gt;...]</code>)</li> <li><code>--input-topics</code>: List of input topics (comma-separated)</li> <li><code>--input-pattern</code>: Pattern of input topics</li> <li><code>--output-topic</code>: The output topic</li> <li><code>--error-topic</code>: A topic to write errors to</li> <li><code>--labeled-input-topics</code>: Additional labeled input topics for different message types (   <code>&lt;String=String&gt;[,&lt;String=String&gt;...]</code>)</li> <li><code>--labeled-input-patterns</code>: Additional labeled input patterns for different message types (   <code>&lt;String=String&gt;[,&lt;String=String&gt;...]</code>)</li> <li><code>--labeled-output-topics</code>: Additional labeled output topics for different message types (   <code>&lt;String=String&gt;[,&lt;String=String&gt;...]</code>)</li> <li><code>--application-id</code>: Unique application ID to use for Kafka Streams. Can also be provided by implementing   <code>StreamsApp#getUniqueAppId()</code></li> <li><code>--volatile-group-instance-id</code>: Whether the group instance id is volatile, i.e., it changes on Streams shutdown</li> </ul> <p>Additional commands:</p> <ul> <li><code>clean</code>: Reset the Kafka Streams application. Additionally, delete the consumer group and all output and intermediate   topics associated with the Kafka Streams application.</li> <li><code>reset</code>: Clear all state stores, consumer group offsets, and internal topics associated with the Kafka Streams   application.</li> </ul>"},{"location":"user/getting-started/setup/#kafka-producer-applications","title":"Kafka Producer Applications","text":"<p>Create a subclass of <code>KafkaProducerApplication</code>.</p> <pre><code>import com.bakdata.kafka.producer.KafkaProducerApplication;\nimport com.bakdata.kafka.producer.ProducerApp;\nimport com.bakdata.kafka.producer.ProducerBuilder;\nimport com.bakdata.kafka.producer.ProducerRunnable;\nimport com.bakdata.kafka.producer.SerializerConfig;\nimport java.util.Map;\nimport org.apache.kafka.clients.producer.Producer;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\npublic class MyProducerApplication extends KafkaProducerApplication&lt;ProducerApp&gt; {\n\n    public static void main(final String[] args) {\n        new MyProducerApplication().startApplication(args);\n    }\n\n    @Override\n    public ProducerApp createApp() {\n        return new ProducerApp() {\n            @Override\n            public ProducerRunnable buildRunnable(final ProducerBuilder builder) {\n                return () -&gt; {\n                    try (final Producer&lt;Object, Object&gt; producer = builder.createProducer()) {\n                        // producer logic\n                    }\n                };\n            }\n\n            @Override\n            public SerializerConfig defaultSerializationConfig() {\n                return new SerializerConfig(StringSerializer.class, StringSerializer.class);\n            }\n\n            @Override\n            public Map&lt;String, Object&gt; createKafkaProperties() {\n                return Map.of(\n                        // additional Kafka properties\n                );\n            }\n        };\n    }\n}\n</code></pre>"},{"location":"user/getting-started/setup/#configuration-options-producer","title":"Configuration Options (Producer)","text":"<p>The following CLI options are available:</p> <ul> <li><code>--bootstrap-servers</code>, <code>--bootstrap-server</code>: List of Kafka bootstrap servers (comma-separated) (required)</li> <li><code>--schema-registry-url</code>: The URL of the Schema Registry</li> <li><code>--kafka-config</code>: Kafka producer configuration (<code>&lt;String=String&gt;[,&lt;String=String&gt;...]</code>)</li> <li><code>--output-topic</code>: The output topic</li> <li><code>--labeled-output-topics</code>: Additional labeled output topics (<code>&lt;String=String&gt;[,&lt;String=String&gt;...]</code>)</li> </ul> <p>Additional commands:</p> <ul> <li><code>clean</code>: Delete all output topics associated with the Kafka producer application.</li> </ul>"},{"location":"user/getting-started/setup/#helm-charts","title":"Helm Charts","text":"<p>For configuration and deployment to Kubernetes, use the provided Helm charts:</p> <ul> <li> <p>Streams applications:</p> <ul> <li>Chart: <code>charts/streams-app</code></li> <li>Example configuration: <code>charts/streams-app/values.yaml</code></li> <li>Cleanup job: <code>charts/streams-app-cleanup-job</code></li> </ul> </li> <li> <p>Producer applications:</p> <ul> <li>Chart: <code>charts/producer-app</code></li> <li>Example configuration: <code>charts/producer-app/values.yaml</code></li> <li>Cleanup job: <code>charts/producer-app-cleanup-job</code></li> </ul> </li> </ul> <p>To configure your streams app, you can use the <code>values.yaml</code> as a starting point. We also provide a chart to clean your streams app.</p>"}]}