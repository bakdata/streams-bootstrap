{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"streams-bootstrap Overview","text":"<p>The <code>streams-bootstrap</code> repository is a comprehensive framework for building, testing, and deploying Kafka Streams and Producer applications with standardized patterns and Kubernetes deployment support.</p> <p>The streams-bootstrap framework simplifies the development lifecycle of Kafka applications by providing base classes, CLI utilities, testing infrastructure, and Helm charts for deployment. For detailed information about the core framework architecture, see Core Framework Architecture. For deployment-specific configuration, see Helm Charts Overview.</p>"},{"location":"#purpose-and-scope","title":"Purpose and Scope","text":"<p><code>streams-bootstrap</code> is a Java framework designed to standardize the development and deployment of Apache Kafka applications.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Application Framework: Base classes and utilities for Kafka Streams and Producer applications</li> <li>CLI Integration: Unified command-line interface for application configuration</li> <li>Testing Infrastructure: Rich test tools for development and CI</li> <li>Kubernetes Deployment: Helm charts for production deployment with auto-scaling and monitoring</li> <li>Lifecycle Management: Cleanup, reset, and topic management support</li> </ul> <p>The framework supports Apache Kafka 4.0 and Java 17, with modules published to Maven Central for easy integration into existing projects.</p>"},{"location":"#framework-architecture","title":"Framework Architecture","text":"<p>The framework uses a modular architecture with clear separation of concerns.</p>"},{"location":"#core-modules","title":"Core Modules Application Types  External Dependencies","text":"<ul> <li><code>streams-bootstrap-core</code>: Base classes like <code>KafkaApplication</code>, <code>Runner</code>, and <code>CleanUpRunner</code></li> <li><code>streams-bootstrap-cli</code>: CLI framework using <code>picocli</code></li> <li><code>streams-bootstrap-test</code>: Testing utilities (<code>TestApplicationRunner</code>, <code>KafkaTestClient</code>)</li> <li><code>streams-bootstrap-large-messages</code>: Support for handling large Kafka messages</li> <li><code>streams-bootstrap-cli-test</code>: CLI test support</li> </ul> <ul> <li><code>KafkaStreamsApplication</code>: Kafka Streams-based apps</li> <li><code>KafkaProducerApplication</code>: Kafka Producer-based apps</li> </ul> <ul> <li>Apache Kafka: <code>kafka-streams</code>, <code>kafka-clients</code></li> <li>Confluent Platform: <code>schema-registry</code>, <code>avro-serde</code></li> <li>Picocli: CLI framework</li> </ul>"},{"location":"#application-types-and-cli-framework","title":"Application Types and CLI Framework","text":"<p>The framework supports two core application types, extending <code>KafkaApplication</code>.</p>"},{"location":"#cli-commands","title":"CLI Commands","text":"<ul> <li><code>run</code>: Run the application</li> <li><code>clean</code>: Delete topics and consumer groups</li> <li><code>reset</code>: Reset internal state and offsets</li> </ul>"},{"location":"#common-cli-configuration-options","title":"Common CLI Configuration Options","text":"<ul> <li><code>--bootstrap-servers</code>: Kafka bootstrap servers (required)</li> <li><code>--schema-registry-url</code>: URL for Avro serialization</li> <li><code>--kafka-config</code>: Key-value Kafka configuration</li> <li><code>--output-topic</code>: Main output topic</li> <li><code>--labeled-output-topics</code>: Named output topics</li> <li><code>--input-topics</code>: Input topics (for Streams apps)</li> <li><code>--input-pattern</code>: Input topic pattern (for Streams apps)</li> <li><code>--application-id</code>: Unique app ID (Streams apps)</li> </ul>"},{"location":"#deployment-and-operations","title":"Deployment and Operations","text":"<p>The framework includes full support for Kubernetes deployments using Helm charts.</p>"},{"location":"#helm-charts","title":"Helm Charts","text":"<ul> <li><code>streams-app</code>: Kafka Streams deployment</li> <li><code>producer-app</code>: Kafka Producer deployment</li> <li><code>streams-app-cleanup-job</code>: Cleanup job for Streams apps</li> <li><code>producer-app-cleanup-job</code>: Cleanup job for Producer apps</li> </ul>"},{"location":"#features_1","title":"Features","text":"<ul> <li>Deployment Modes: Job, CronJob, or Deployment based on application type</li> <li>Auto-scaling: KEDA integration for Kafka lag-based scaling</li> <li>Monitoring: JMX metrics export and Prometheus integration</li> <li>State Storage: Persistent volumes for Kafka Streams state stores</li> <li>Security: Secret management and service account configuration</li> </ul>"},{"location":"#testing-features","title":"Testing Features","text":"<ul> <li>Test Application Runner: Simplified test execution with automatic configuration</li> <li>Kafka Test Client: Utilities for topic and consumer group management in tests</li> <li>Consumer Group Verification: Validation of consumer group state and offsets</li> <li>Schema Registry Mock: In-memory schema registry for testing serialization</li> <li></li> </ul>"},{"location":"#summary","title":"Summary","text":"<p>streams-bootstrap provides a complete framework for Kafka application development with standardized patterns for configuration, deployment, and testing. The modular architecture allows developers to use only the components they need while maintaining consistency across different application types and deployment environments.</p> <p>The framework's key strengths include:</p>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>Standardized CLI: Consistent and predictable configuration</li> <li>Production-ready Deployment: Helm charts with monitoring and scaling</li> <li>Robust Testing Support: Tools for unit and integration tests</li> <li>Lifecycle Management: Easy cleanup and reset operations</li> </ul> <p>Modular by design, it enables developers to use only what they need\u2014while maintaining operational consistency across deployments.</p>"},{"location":"app-lifecycle/","title":"Application Lifecycle Management Test","text":"<p>This section documents the lifecycle management aspects of applications built with streams-bootstrap, focusing on how applications are started, stopped, cleaned up, and reset. The framework provides standardized ways to manage the complete lifecycle of both Kafka Streams and Producer applications.</p> <p>For information about creating Kafka Streams applications, see Kafka Streams Applications. For information about creating Kafka Producer applications, see Kafka Producer Applications.</p>"},{"location":"app-lifecycle/#application-lifecycle-states","title":"Application Lifecycle States","text":"<p>Applications built with streams-bootstrap follow a defined lifecycle with specific states and transitions:</p> <ul> <li>Deployed</li> <li>Running</li> <li>Processing</li> <li>Cleanup</li> </ul>"},{"location":"app-lifecycle/#running-applications","title":"Running Applications","text":"<p>Applications built with streams-bootstrap can be started in two primary ways:</p> <ul> <li>Via Command Line Interface: When deployed as a container, the application's <code>run</code> command is the default   entrypoint.</li> <li>Programmatically: In code, the <code>run()</code> method or appropriate runner class can be used.</li> </ul>"},{"location":"app-lifecycle/#running-streams-applications","title":"Running Streams Applications","text":"<p>Streams applications start Kafka Streams processing through the <code>StreamsRunner</code> class, which handles the lifecycle of a Kafka Streams instance.</p> <p>When an application is running, it processes records from the input topics and writes results to the output topic. The application remains in the running state until explicitly stopped or encounters an unrecoverable error.</p>"},{"location":"app-lifecycle/#running-producer-applications","title":"Running Producer Applications","text":"<p>Producer applications use the <code>ProducerRunner</code> class, which executes a runnable defined by the application:</p> <p>Unlike Streams applications, Producer applications typically run to completion and then terminate, unless configured as a continuous service.</p>"},{"location":"app-lifecycle/#cleaning-up-applications","title":"Cleaning Up Applications","text":"<p>The framework provides a built-in mechanism to clean up all resources associated with an application.</p>"},{"location":"app-lifecycle/#what-gets-cleaned","title":"What Gets Cleaned","text":"<p>When the cleanup operation is triggered, the following resources are removed:</p> Resource Type Description Streams Apps Producer Apps Output Topics The main output topic of the application \u2713 \u2713 Intermediate Topics Topics for stream operations like <code>through()</code> \u2713 N/A Internal Topics Topics for state stores or repartitioning (Kafka Streams) \u2713 N/A Consumer Groups Consumer group metadata \u2713 N/A Schema Registry All registered schemas \u2713 \u2713"},{"location":"app-lifecycle/#triggering-cleanup","title":"Triggering Cleanup","text":"<p>Cleanup can be triggered:</p> <ul> <li>Via Command Line: Helm cleanup jobs</li> <li>Programmatically:</li> </ul> <pre><code>// For streams applications\ntry(StreamsCleanUpRunner cleanUpRunner=streamsApp.createCleanUpRunner()){\n        cleanUpRunner.clean();\n        }\n\n// For producer applications\n        try(CleanUpRunner cleanUpRunner=producerApp.createCleanUpRunner()){\n        cleanUpRunner.clean();\n        }\n</code></pre> <p>The framework ensures that cleanup operations are idempotent, meaning they can be safely retried without causing additional issues.</p>"},{"location":"app-lifecycle/#resetting-stream-applications","title":"Resetting Stream Applications","text":"<p>For Kafka Streams applications, the framework provides a special \"reset\" operation that preserves the application but resets its processing state. This is useful for reprocessing data without having to recreate the application's resources.</p>"},{"location":"app-lifecycle/#what-gets-reset","title":"What Gets Reset","text":"<p>When reset is triggered, the following resources are affected:</p> Resource Action State Stores Cleared locally, changelog topics deleted Internal Topics Deleted (e.g. repartition topics) Consumer Offsets Reset to earliest for input topics Output Topic Preserved Application Config Preserved"},{"location":"app-lifecycle/#triggering-reset","title":"Triggering Reset","text":"<p>Via Command Line (when deployed with Helm charts) or programmatically:</p> <pre><code>try(StreamsCleanUpRunner cleanUpRunner=streamsApp.createCleanUpRunner()){\n        cleanUpRunner.reset();\n        }\n</code></pre> <p>After reset, the application can be started again and will reprocess all data from the beginning, effectively providing a \"replay\" capability while maintaining the original application structure.</p>"},{"location":"app-lifecycle/#error-handling","title":"Error Handling","text":"<p>The framework provides default handling for common errors:</p> <ul> <li>Missing Input Topics: Handled gracefully at startup</li> <li>Consumer Group Errors: Safe during cleanup</li> <li>Schema Registry Errors: Handled during registry operations</li> </ul> <p>Kafka Streams-specific errors are managed via <code>StateListener</code> and <code>StreamsUncaughtExceptionHandler</code>.</p>"},{"location":"app-lifecycle/#custom-cleanup-hooks","title":"Custom Cleanup Hooks","text":"<p>Applications can register custom cleanup logic.</p> <p>Useful for removing external metadata or triggering system-side notifications.</p>"},{"location":"producer-app/","title":"Kafka Producer Applications","text":"<p>This document provides technical documentation for Kafka Producer Applications within the <code>streams-bootstrap</code> framework. Kafka Producer Applications extend the <code>KafkaApplication</code> base class to create applications that produce messages to Kafka topics with standardized CLI options, configuration management, and deployment patterns.</p>"},{"location":"producer-app/#overview","title":"Overview","text":"<p>Kafka Producer Applications in <code>streams-bootstrap</code> provide a structured framework for building applications that produce data to Kafka topics. The framework handles:</p> <ul> <li>CLI argument parsing</li> <li>Kafka configuration</li> <li>Schema Registry integration</li> <li>Resource lifecycle management via the <code>KafkaApplication</code> base class</li> </ul>"},{"location":"producer-app/#architecture","title":"Architecture","text":""},{"location":"producer-app/#core-components-and-responsibilities","title":"Core Components and Responsibilities","text":"<ul> <li>KafkaApplication: Abstract base with lifecycle methods</li> <li>KafkaProducerApplication: Implementation with producer-specific lifecycle</li> <li>ProducerApp: Interface defining producer logic and configuration</li> <li>ConfiguredProducerApp: Wraps app logic with configuration</li> <li>ExecutableProducerApp: Responsible for running and cleaning up</li> <li>ProducerRunner: Executes the producer</li> <li>ProducerCleanUpRunner: Handles resource cleanup</li> <li>ProducerRunnable: User-defined message production logic</li> </ul>"},{"location":"producer-app/#creating-a-kafka-producer-application","title":"Creating a Kafka Producer Application","text":""},{"location":"producer-app/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Extend <code>KafkaProducerApplication</code> with your specific app type</li> <li>Implement <code>createApp()</code> to define your producer logic</li> <li>Implement <code>buildRunnable()</code> inside <code>ProducerApp</code></li> <li>Configure serialization using <code>defaultSerializationConfig()</code></li> </ol>"},{"location":"producer-app/#implementation-example","title":"Implementation Example","text":""},{"location":"producer-app/#basic-producer-application","title":"Basic Producer Application","text":"<pre><code>public class MyProducerApplication extends KafkaProducerApplication&lt;ProducerApp&gt; {\n\n    public static void main(final String[] args) {\n\n        new MyProducerApplication().startApplication(args);\n    }\n\n    @Override\n    public ProducerApp createApp() {\n        return new ProducerApp() {\n            @Override\n            public ProducerRunnable buildRunnable(final ProducerBuilder builder) {\n                return () -&gt; {\n                    try (final Producer&lt;Object, Object&gt; producer = builder.createProducer()) {\n                        producer.send(new ProducerRecord&lt;&gt;(\n                                builder.getTopics().getOutputTopic(), \"key\", \"value\"\n                        ));\n                    }\n                };\n            }\n\n            @Override\n            public SerializerConfig defaultSerializationConfig() {\n                return new SerializerConfig(StringSerializer.class, StringSerializer.class);\n            }\n        };\n    }\n}\n</code></pre>"},{"location":"producer-app/#using-simplekafkaproducerapplication","title":"Using <code>SimpleKafkaProducerApplication</code>","text":"<p>For programmatic usage without custom CLI:</p> <pre><code>try(final KafkaProducerApplication&lt;?&gt; app=new SimpleKafkaProducerApplication&lt;&gt;(()-&gt;\n        new ProducerApp(){\n@Override\npublic ProducerRunnable buildRunnable(final ProducerBuilder builder){\n        return()-&gt;{\n        try(final Producer&lt;Object, Object&gt; producer=builder.createProducer()){\n// Producer logic\n        }\n        };\n        }\n\n@Override\npublic SerializerConfig defaultSerializationConfig(){\n        return new SerializerConfig(StringSerializer.class,StringSerializer.class);\n        }\n        }\n        )){\n        app.setBootstrapServers(\"localhost:9092\");\n        app.setOutputTopic(\"output-topic\");\n        app.run();\n        }\n</code></pre>"},{"location":"producer-app/#configuration-options","title":"Configuration Options","text":""},{"location":"producer-app/#cli-arguments","title":"CLI Arguments","text":"<p>Kafka Producer applications inherit configuration options from KafkaApplication and support the following CLI arguments:</p> <pre><code>--bootstrap-servers         Kafka bootstrap servers (comma-separated)          (Required)\n--bootstrap-server          Alias for --bootstrap-servers                      (Required)\n--schema-registry-url       URL for Avro schema registry                       (Optional)\n--kafka-config              Additional Kafka config (key=value,...)            (Optional)\n--output-topic              Main Kafka topic to produce to                     (Optional)\n--labeled-output-topics     Named output topics (label1=topic1,...)            (Optional)\n</code></pre>"},{"location":"producer-app/#environment-variable-support","title":"Environment Variable Support","text":"<p>The framework automatically parses environment variables with the <code>APP_ prefix</code> (configurable via <code>ENV_PREFIX</code>). Environment variables are converted to CLI arguments:</p> <pre><code>APP_BOOTSTRAP_SERVERS       \u2192 --bootstrap-servers\nAPP_SCHEMA_REGISTRY_URL     \u2192 --schema-registry-url\nAPP_OUTPUT_TOPIC            \u2192 --output-topic\n</code></pre> <p>Additionally, Kafka-specific environment variables with the <code>KAFKA_</code> prefix are automatically added to the Kafka configuration.</p>"},{"location":"producer-app/#configuration-precedence","title":"Configuration Precedence","text":"<p>Kafka properties are merged in the following order (later values override earlier ones):</p> <ol> <li>Base configuration</li> <li>App config from ProducerApp.createKafkaProperties()</li> <li>Environment variables (<code>KAFKA_</code>)</li> <li>Runtime args (--bootstrap-servers, etc.)</li> <li>Serialization config from ProducerApp.defaultSerializationConfig()</li> <li>CLI overrides via --kafka-config</li> </ol>"},{"location":"producer-app/#serialization-configuration","title":"Serialization Configuration","text":"<p>Producer applications specify key and value serializers via the defaultSerializationConfig() method in your ProducerApp implementation:</p> <pre><code>@Override\npublic SerializerConfig defaultSerializationConfig(){\n        return new SerializerConfig(StringSerializer.class,SpecificAvroSerializer.class);\n        }\n</code></pre> <p>Common serializer configurations:</p> Key Serializer Value Serializer Use Case StringSerializer StringSerializer Simple string messages StringSerializer SpecificAvroSerializer Avro schema evolution StringSerializer GenericAvroSerializer Dynamic Avro schemas ByteArraySerializer ByteArraySerializer Binary data"},{"location":"producer-app/#custom-kafka-properties","title":"Custom Kafka Properties","text":"<p>Override createKafkaProperties() to add custom producer configuration:</p> <pre><code>@Override\npublic Map&lt;String, Object&gt; createKafkaProperties(){\n        return Map.of(\n        ProducerConfig.ACKS_CONFIG,\"all\",\n        ProducerConfig.RETRIES_CONFIG,3,\n        ProducerConfig.BATCH_SIZE_CONFIG,16384,\n        ProducerConfig.LINGER_MS_CONFIG,5\n        );\n        }\n</code></pre>"},{"location":"producer-app/#schema-registry-integration","title":"Schema Registry Integration","text":"<p>When <code>--schema-registry-url</code> is set:</p> <ul> <li>Schema registration happens automatically</li> <li>Schema cleanup is handled during clean</li> <li>Schema evolution is supported</li> </ul> <pre><code>props.put(KafkaAvroSerializerConfig.SCHEMA_REGISTRY_URL_CONFIG,schemaRegistryUrl);\n</code></pre>"},{"location":"producer-app/#resource-cleanup","title":"Resource Cleanup","text":""},{"location":"producer-app/#command","title":"Command","text":"<p>The clean command provides comprehensive resource cleanup for producer applications:</p> <pre><code>java -jar my-producer-app.jar --bootstrap-servers localhost:9092 --output-topic my-topic clean\n</code></pre>"},{"location":"producer-app/#operations","title":"Operations","text":"<ul> <li>Delete output topics</li> <li>Delete schemas</li> <li>Run custom cleanup hooks via ProducerApp.setupCleanUp()</li> </ul>"},{"location":"producer-app/#custom-hook","title":"Custom Hook","text":"<p>Producer applications can register custom cleanup logic:</p> <pre><code>@Override\npublic void setupCleanUp(final EffectiveAppConfiguration configuration){\n        configuration.addCleanupHook(()-&gt;{\n// Your cleanup logic\n        });\n        }\n</code></pre>"},{"location":"producer-app/#kubernetes-deployment","title":"Kubernetes Deployment","text":""},{"location":"producer-app/#helm-charts","title":"Helm Charts","text":"<ul> <li><code>producer-app</code>: Main deployment</li> <li><code>producer-app-cleanup-job</code>: Cleanup job</li> </ul>"},{"location":"producer-app/#deployment-modes","title":"Deployment Modes","text":"Mode Use Case Resource Type Deployment Long-running producer apps/v1/Deployment Job One-time run batch/v1/Job CronJob Scheduled job batch/v1/CronJob"},{"location":"producer-app/#configuration","title":"Configuration","text":"<p>Values from <code>values.yaml</code> are converted into:</p> <ul> <li>CLI arguments</li> <li>Environment variables</li> <li>Kafka properties</li> </ul>"},{"location":"streams-app/","title":"Kafka Streams Applications","text":""},{"location":"streams-app/#purpose-and-scope","title":"Purpose and Scope","text":"<p>This page describes how to create and configure Kafka Streams applications using the streams-bootstrap framework. It covers the core classes, configuration options, and lifecycle management for Kafka Streams applications specifically. For information about Kafka Producer applications, see Kafka Producer Applications. For details about application lifecycle management, see Application Lifecycle Management.</p>"},{"location":"streams-app/#core-streams-classes","title":"Core Streams Classes","text":""},{"location":"streams-app/#kafkastreamsapplication","title":"KafkaStreamsApplication","text":"<p><code>KafkaStreamsApplication&lt;T extends StreamsApp&gt;</code> is the abstract base class for creating Kafka Streams applications. It provides command-line options for configuring the application and methods for managing the application lifecycle.</p> <p>Key responsibilities:</p> <ul> <li>Create and configure a StreamsApp instance</li> <li>Parse command-line arguments and environment variables</li> <li>Manage application lifecycle (run, clean, reset)</li> <li>Configure error handling and state listeners</li> </ul>"},{"location":"streams-app/#streamsapp-interface","title":"StreamsApp Interface","text":"<p>The <code>StreamsApp</code> interface defines the contract for implementing a Kafka Streams application:</p>"},{"location":"streams-app/#creating-a-kafka-streams-application","title":"Creating a Kafka Streams Application","text":"<p>To create a Kafka Streams application, you need to:</p> <ol> <li>Create a class that extends <code>KafkaStreamsApplication</code></li> <li>Implement the <code>createApp()</code> method to return a <code>StreamsApp</code> implementation</li> <li>Define the topology in the <code>buildTopology()</code> method</li> <li>Define a unique application ID in the <code>getUniqueAppId()</code> method</li> </ol> <ul> <li>[ ] Add link to hands on example or just remove and add as separate page</li> </ul> <p>Simplified Structure:</p>"},{"location":"streams-app/#simplekafkastreamsapplication","title":"SimpleKafkaStreamsApplication","text":"<p>For simple use cases, the framework provides a <code>SimpleKafkaStreamsApplication&lt;T&gt;</code> implementation that takes a supplier function for creating your <code>StreamsApp</code> instance.</p>"},{"location":"streams-app/#configuration-options","title":"Configuration Options","text":"<p>The <code>KafkaStreamsApplication</code> class provides several configuration options through command-line arguments:</p> Option Description Default --bootstrap-servers List of Kafka bootstrap servers (comma-separated) Required --schema-registry-url The URL of the Schema Registry None --kafka-config Kafka Streams config ([,...]) None --input-topics List of input topics (comma-separated) Empty list --input-pattern Pattern of input topics None --output-topic The output topic None --error-topic A topic to write errors to None --labeled-input-topics Labeled input topics with different message types None --labeled-input-patterns Additional labeled input patterns None --labeled-output-topics Additional labeled output topics with different message types None --application-id Unique application ID for Kafka Streams Auto-generated --volatile-group-instance-id Whether the group instance id is volatile false"},{"location":"streams-app/#configuration-priority","title":"Configuration Priority","text":"<p>Kafka configuration follows this order of precedence (highest to lowest):</p> <ol> <li>Command-line parameters</li> <li>Environment variables (prefixed with <code>KAFKA_</code>)</li> <li>Configuration provided by the <code>StreamsApp.createKafkaProperties()</code> method</li> <li>Framework default configuration</li> </ol> <p>Defaults set by framework:</p> <pre><code>processing.guarantee=exactly_once_v2\nproducer.max.in.flight.requests.per.connection=1\nproducer.acks=all\nproducer.compression.type=gzip\n</code></pre>"},{"location":"streams-app/#application-lifecycle","title":"Application Lifecycle","text":""},{"location":"streams-app/#running-the-application","title":"Running the Application","text":"<p>To run a Kafka Streams application:</p> <pre><code>public static void main(final String[] args) {\n    new MyStreamsApplication().startApplication(args);\n}\n</code></pre> <p>The framework internally:</p> <ul> <li>Parses arguments</li> <li>Creates a <code>StreamsApp</code> instance</li> <li>Wraps it in <code>ConfiguredStreamsApp</code></li> <li>Converts to <code>ExecutableStreamsApp</code></li> <li>Runs via <code>StreamsRunner</code></li> </ul> <ul> <li>[ ] add either diagram or more input on Streams app above for more context </li> </ul>"},{"location":"streams-app/#cleaning-up-resources","title":"Cleaning Up Resources","text":"<p>Built-in commands:</p> <ul> <li><code>clean</code>: Deletes consumer group, output and intermediate topics</li> </ul> <pre><code>java -jar my-app.jar clean\n</code></pre> <ul> <li><code>reset</code>: Clears state stores, offsets, and internal topics</li> </ul> <pre><code>java -jar my-app.jar reset\n</code></pre> <p>reset command</p> <ul> <li><code>reset()</code><ul> <li>Reset state stores</li> <li>Reset consumer offsets</li> <li>Delete internal topics</li> </ul> </li> </ul> <p>clean command</p> <ul> <li><code>clean()</code><ul> <li>Delete output topics</li> <li>Delete internal topics</li> <li>Delete intermediate topics</li> <li>Delete consumer group</li> <li>Delete schemas</li> </ul> </li> </ul>"},{"location":"streams-app/#advanced-features","title":"Advanced Features","text":""},{"location":"streams-app/#error-handling","title":"Error Handling","text":"<p>You can override error handling:</p> <pre><code>@Override\nprotected StreamsUncaughtExceptionHandler createUncaughtExceptionHandler() {\n    return new MyCustomExceptionHandler();\n}\n</code></pre>"},{"location":"streams-app/#state-listeners","title":"State Listeners","text":"<p>Monitor state transitions:</p> <pre><code>@Override\nprotected StateListener createStateListener() {\n    return new MyCustomStateListener();\n}\n</code></pre>"},{"location":"streams-app/#on-start-hook","title":"On-Start Hook","text":"<p>Execute logic after Kafka Streams has started:</p> <pre><code>@Override\nprotected void onStreamsStart(final RunningStreams runningStreams) {\n    // Custom setup\n}\n</code></pre>"},{"location":"streams-app/#deployment","title":"Deployment","text":"<p>For deploying your Kafka Streams applications to Kubernetes, the framework provides Helm charts that can be found in the <code>charts/</code> directory. The configuration and deployment aspects are covered in detail in Helm Charts Overview and the streams-app Chart.</p>"},{"location":"testing/","title":"Testing Framework","text":""},{"location":"testing/#overview","title":"Overview","text":"<p>The <code>streams-bootstrap</code> Testing Framework provides a comprehensive set of tools for testing Kafka Streams and Producer applications built with the <code>streams-bootstrap</code> library. This framework simplifies both unit and integration testing by providing test abstractions that handle Kafka infrastructure setup, Schema Registry integration, and consumer group verification.</p> <p>The framework supports testing with real Kafka clusters using TestContainers, mock Schema Registry for schema-aware testing, and utilities for verifying application behavior and consumer group states.</p> <p>For information about creating Kafka Streams applications that you might want to test, see Kafka Streams Applications .</p>"},{"location":"testing/#core-testing-components","title":"Core Testing Components","text":""},{"location":"testing/#kafkatest-base-class","title":"KafkaTest Base Class","text":"<p><code>KafkaTest</code> is an abstract base class that sets up a Kafka environment using TestContainers. It provides:</p> <ul> <li>Kafka container setup</li> <li>Access to bootstrap servers and Schema Registry</li> <li>Methods for waiting on consumer group states</li> <li>Integration with <code>TestSchemaRegistry</code></li> <li>Creation of <code>KafkaTestClient</code> instances</li> </ul>"},{"location":"testing/#kafkatestclient","title":"KafkaTestClient","text":"<p><code>KafkaTestClient</code> is a fluent test client that simplifies:</p> <ul> <li>Producing data</li> <li>Consuming records</li> <li>Admin operations</li> <li>Topic creation and verification</li> </ul>"},{"location":"testing/#consumergroupverifier","title":"ConsumerGroupVerifier","text":"<p>Provides tools to:</p> <ul> <li>Check if a group is active or closed</li> <li>Get current group state</li> <li>Verify processing completion (lag = 0)</li> <li>Compute lag manually</li> </ul>"},{"location":"testing/#unit-testing-with-fluent-kafka-streams-tests","title":"Unit Testing with <code>fluent-kafka-streams-tests</code>","text":"<p>The framework integrates with <code>fluent-kafka-streams-tests</code> for unit testing Kafka Streams topologies.</p>"},{"location":"testing/#dependencies","title":"Dependencies:","text":"Dependency Purpose <code>fluent-kafka-streams-tests</code> Kafka Streams unit testing <code>junit-jupiter</code> JUnit 5 test runner <code>testcontainers-kafka</code> Kafka container for integration testing <code>mockito-core</code> Mocking framework <code>assertj-core</code> Fluent assertions <code>awaitility</code> Polling/waiting utility"},{"location":"testing/#testschemaregistry","title":"TestSchemaRegistry","text":"<p><code>TestSchemaRegistry</code> provides built-in support for Schema Registry in tests using a mock implementation. It creates isolated Schema Registry instances for testing schema-aware applications.</p>"},{"location":"testing/#features","title":"Features:","text":"<ul> <li>Random scoped mock URLs to avoid collisions</li> <li>Support for custom mock URLs</li> <li>Configurable schema providers</li> <li>Compatible with Confluent\u2019s <code>MockSchemaRegistry</code></li> </ul>"},{"location":"testing/#example","title":"Example:","text":"<pre><code>// Random scope\nTestSchemaRegistry registry=new TestSchemaRegistry();\n\n// Custom scope\n        TestSchemaRegistry registry=new TestSchemaRegistry(\"mock://custom-scope\");\n\n// Default providers\n        SchemaRegistryClient client=registry.getSchemaRegistryClient();\n\n// With custom providers\n        List&lt;SchemaProvider&gt; providers=List.of(new ProtobufSchemaProvider());\n        SchemaRegistryClient client=registry.getSchemaRegistryClient(providers);\n</code></pre>"},{"location":"testing/#integration-testing-with-testcontainers","title":"Integration Testing with TestContainers","text":"<p>For integration tests that require a real Kafka environment, the framework provides integration with TestContainers.</p>"},{"location":"testing/#single-node-kafka-testing","title":"Single Node Kafka Testing","text":"<p>KafkaTest provides a base class for integration tests with a single Kafka broker:</p> <pre><code>\n@Testcontainers\npublic abstract class KafkaTest {\n    @Container\n    private final KafkaContainer kafkaCluster = newCluster();\n\n    public static KafkaContainer newCluster() {\n        return new KafkaContainer(DockerImageName.parse(\"apache/kafka\")\n                .withTag(AppInfoParser.getVersion()));\n    }\n}\n</code></pre> <p>This creates a Kafka container using TestContainers with the Apache Kafka image matching the client version.</p>"},{"location":"testing/#multi-node-cluster-testing","title":"Multi-Node Cluster Testing","text":"<p>For testing with multi-node Kafka clusters, the framework provides <code>ApacheKafkaContainerCluster</code>:</p> <ul> <li>[] INSERT IMAGE</li> </ul> <p>Example usage:</p> <pre><code>ApacheKafkaContainerCluster cluster=new ApacheKafkaContainerCluster(\"3.4.0\",3,2);\n        cluster.start();\n        String bootstrapServers=cluster.getBootstrapServers();\n// Run tests...\n        cluster.stop();\n</code></pre>"},{"location":"testing/#features_1","title":"Features:","text":"<ul> <li>Configurable broker count</li> <li>Configurable replication factor for internal topics</li> <li>Uses KRaft (no ZooKeeper)</li> <li>Waits for all brokers to be ready before returning</li> </ul>"},{"location":"testing/#utilities-for-kafka-testing","title":"Utilities for Kafka Testing","text":""},{"location":"testing/#kafkatestclient-operations","title":"KafkaTestClient Operations","text":"<p><code>KafkaTestClient</code> provides a fluent API for common Kafka operations in tests:</p>"},{"location":"testing/#topic-management","title":"Topic Management","text":"<pre><code>KafkaTestClient client=newTestClient();\n\n// Create topic with default settings (1 partition, 1 replica)\n        client.createTopic(\"my-topic\");\n\n// Create topic with custom settings\n        client.createTopic(\"my-topic\",\n        KafkaTestClient.defaultTopicSettings()\n        .partitions(3)\n        .replicationFactor((short)1)\n        .build());\n\n// Create topic with config\n        Map&lt;String, String&gt; config=Map.of(\"cleanup.policy\",\"compact\");\n        client.createTopic(\"my-topic\",settings,config);\n\n// Check if topic exists\n        boolean exists=client.existsTopic(\"my-topic\");\n</code></pre>"},{"location":"testing/#data-production","title":"Data Production","text":"<pre><code>client.send()\n        .withKeySerializer(new StringSerializer())\n        .withValueSerializer(new StringSerializer())\n        .to(\"topic-name\",List.of(\n        new SimpleProducerRecord&lt;&gt;(\"key1\",\"value1\"),\n        new SimpleProducerRecord&lt;&gt;(\"key2\",\"value2\")\n        ));\n</code></pre>"},{"location":"testing/#data-consumption","title":"Data Consumption","text":"<pre><code>List&lt;ConsumerRecord&lt;String, String&gt;&gt;records=client.read()\n        .withKeyDeserializer(new StringDeserializer())\n        .withValueDeserializer(new StringDeserializer())\n        .from(\"topic-name\",Duration.ofSeconds(10));\n</code></pre>"},{"location":"testing/#administrative-operations","title":"Administrative Operations","text":"<p><code>KafkaTestClient</code> provides access to administrative operations through AdminClientX:</p> <pre><code>try(AdminClientX admin=client.admin()){\n        TopicClient topicClient=admin.getTopicClient();\n        ConsumerGroupClient consumerGroupClient=admin.getConsumerGroupClient();\n        }\n</code></pre>"},{"location":"testing/#test-configuration","title":"Test Configuration","text":""},{"location":"testing/#runtime-configuration","title":"Runtime Configuration","text":"<p>The framework provides preconfigured settings for Kafka applications in test environments through the <code>KafkaTest</code> base class.</p> <pre><code>// No schema registry\nRuntimeConfiguration config=createConfigWithoutSchemaRegistry();\n\n// With schema registry\n        RuntimeConfiguration config=createConfig();\n</code></pre>"},{"location":"testing/#constants","title":"Constants","text":"Setting Value Purpose <code>POLL_TIMEOUT</code> 10 seconds Wait for consuming <code>SESSION_TIMEOUT</code> 10 seconds Consumer session timeout <p>Custom test setup example:</p> <pre><code>RuntimeConfiguration config=RuntimeConfiguration.create(getBootstrapServers())\n        .withNoStateStoreCaching()\n        .withSessionTimeout(SESSION_TIMEOUT);\n</code></pre>"},{"location":"testing/#consumer-group-verification","title":"Consumer Group Verification","text":"<p>The framework provides utilities for verifying consumer group states:</p> <pre><code>// Wait for application to become active\nawaitActive(app);\n\n// Wait for completion of processing\n        awaitProcessing(app);\n\n// Wait for app to shut down\n        awaitClosed(app);\n</code></pre> <p>These methods ensure test reliability by validating consumer group behavior via <code>ConsumerGroupVerifier</code>.</p>"}]}